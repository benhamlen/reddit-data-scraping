{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.1.4-py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting update-checker>=0.18\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Collecting websocket-client>=0.54.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting prawcore<2.0,>=1.5.0\n",
      "  Downloading prawcore-1.5.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/conda/lib/python3.8/site-packages (from prawcore<2.0,>=1.5.0->praw) (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.5.0->praw) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.5.0->praw) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.5.0->praw) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.5.0->praw) (1.26.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
      "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
      "Successfully installed praw-7.1.4 prawcore-1.5.0 update-checker-0.18.0 websocket-client-0.57.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QVVSpaWI7k_v0Q <br>\n",
    "h6WFiUCnSh6teizjnv_6dtJXauDvmQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = ''\n",
    "client_secret = ''\n",
    "user_agent = \"crypticbtr\"\n",
    "\n",
    "reddit = praw.Reddit(client_id=\"QVVSpaWI7k_v0Q\",\n",
    "                     client_secret=\"h6WFiUCnSh6teizjnv_6dtJXauDvmQ\",\n",
    "                     password=\"TquickBfoxJoverTlazyD_\",\n",
    "                     user_agent=\"testscript by u/crypticbtr\",\n",
    "                     username=\"CrypticBTR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where do you read the news of ML research? As the field is rapidly growing, I find it difficult to keep up with the research. I'm mainly interested in NLP although I'm also seeking for a AI/ML/DL news source.\n",
      "\n",
      "I'd also like to know any other websites/aggregator you visit daily or weekly both purely scientific (paper reviews) and popular science.\n",
      "What tooling do you use for custom image tagging and labelling? \n",
      " \n",
      "I have a bunch of unique imagines and want something thats lean, well documented, and plays nice with current languages/libraries/data formats. \n",
      "\n",
      "What are your goto's, tried and trusted?\n",
      "In section 14.1 \\[0\\] of the Deep Learning book it says that \"When the decoder is linear and L is the mean squared error, an undercomplete autoencoder learns to span the same subspace as PCA\". I could not find a source/proof for this statement. \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "This is clearly exactly PCA with a one layer encoder and one layer decoder. But the connection with multi-layer autoencoders is not clear to me. If someone could explain or point me in the right direction where to read about the above statement that would be great!\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\[0\\] [https://www.deeplearningbook.org/contents/autoencoders.html](https://www.deeplearningbook.org/contents/autoencoders.html)\n",
      "Is the Book \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio and Aaron Courville a good resource in 2020 or are there other, better ones? I'd like to hear some opinions and suggestions otherwise.\n",
      "Is [the mathematics for machine learning specialization offered by Imperial College London](https://www.coursera.org/specializations/mathematics-machine-learning) sufficient for maths for machine learning from someone from non-cs background. Specially, I am taking the Machine Learning course by Andrew Ng.\n",
      "In the context of artificial general intelligence, what do you think is the most viable alternative to today's commonly used artificial neural network architecture if computational resources and speed isn't (too much of) an issue? (e.g. Numenta's SDR, Deep Bayesian, hybrid architecture of sort, etc.)\n",
      "How do I clone a git repository and start using predefined models to generate sounds/music/noises, etc...\n",
      "\n",
      "I would like to train a model using existing ASMR noises to generate new sounds. Something like this video here: [https://youtu.be/w926Afa1HbY](https://youtu.be/w926Afa1HbY)\n",
      "\n",
      "Also, any recommendations on what to use? Magenta, MelGan, or something else?\n",
      "So I am currently watching lessons from Machine Learning course by Andrew NG in Coursera, in week 6 he first talks about splitting the data set into 2 parts where these two parts are training set and test set, then selects the best fitting hypothesis function according to the error rate he got on these different functions.   \n",
      "After this video, he talks about Cross Validation Set. Where now he splits the data set into 3 parts where there are now Training Set, Cross Validation Set and Test Set. He then explains that it is better to use error rates that got by Cross Validation Set, but I wasn't able to get the idea that why it is better to select hypothesis function using the error rates that we got by Cross Validation Set. \n",
      "\n",
      "I tried to search about it but I think since the Cross Validation Set I learned in the course is very simple I got confused by the extra terms (like k-folding etc). Can someone help me to understand why it is better over just using two sets (training and test) ?\n",
      "Who is lucidrains/Phil Wang ([https://github.com/lucidrains](https://github.com/lucidrains) / [https://twitter.com/lucidrains?lang=en](https://twitter.com/lucidrains?lang=en))?  For context, he authors and maintains a variety of repositories related to state-of-the-art attention models. They're written in a clear, simple and modular way, and many times are released in minutes or hours from when papers are first released.\n",
      "\n",
      "Is he a student/graduate at a research institution/company? How can he write code so efficiently? Is there any methodology or trick to becoming so proficient in reading a paper, digesting it immediately, and then putting it into code?\n",
      "I am not sure if my question fits more in /r/cscareerquestions/ \n",
      "\n",
      "I will start a bachelor degree in september next year here in switzerland (BSC in AI & ML). I am 25M with about 4 years job experience in IT, what are some useful prep courses online or with books that I can start now.\n",
      "\n",
      "I want to finish my degree with honors while continuing to work 60% at my current workplace. Currently Im studying linear algebra with https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/syllabus/ (is this outdated?) and for python I would start with Jose Portilla's Udemy courses as soon as they get discounted again.\n",
      "\n",
      "I still need something to learn statistics 6 with, the problem I have is that I dont know what the 6 means. Does it mean 6th grade or 6th part of something?\n",
      "\n",
      "Any other recommendations of things I can do to prepare? This is the schools bachelor  [overview page](https://www.hslu.ch/en/lucerne-school-of-information-technology/degree-programs/bachelor/artificial-intelligence-and-machine-learning/) and the required  [modules](https://www.hslu.ch/en/lucerne-school-of-information-technology/degree-programs/bachelor/artificial-intelligence-and-machine-learning/module/).\n",
      "\n",
      "Thank you for your time.\n",
      "\n",
      "I will most likely follow this  [reddit post](https://www.reddit.com/r/learnmachinelearning/comments/jv4yrk/a_roadmap_to_andrew_ngs_cs229/)\n",
      "About Categorical Crossentropy loss: Wouldn't simply predicting a 1 for all classes always lead to zero loss, since log(1)==0?\n",
      "\n",
      "Is it just the assumption that a soft max layer before that avoids cheating in that way, and that makes Categorical CE work?\n",
      "I have never used ML but am supposed to cut my teeth on a project at work using ML soon. I will be given a set of photos (about 15k) taken from security like cameras at about 10 locations taking photos of a wetland area (marsh/lagoon). The cameras take photos over consistent intervals of time. My task is to build a ML model that can look at the photos and count how many birds are in each photo. It does not need to identify species of birds, just return a count of birds in each photo. \n",
      "\n",
      "My question is what type of machine learning will I be using? I have looked online and it seems like I will need to use Image Localization. While I don't necessarily need to have highly accurate boxes around the birds, I am assuming once it bounds all the birds in a photo into boxes it is very easy to retrieve counts. Is this accurate or is there another type of model I will use? I should also note I am proficient with python and will be using python primarily for this project.\n",
      "\n",
      "In addition to any feedback on my question, I would really appreciate it if anyone had any good learning resources. I am considering buying [this book](https://machinelearningmastery.com/deep-learning-for-computer-vision/), but if anyone has any other/better recommendations I would really appreciate it! \n",
      "\n",
      "Thank you in advance for your time!\n",
      "I'm v. new to Tensorflow. Is there any way of predicting what operations are fast, other than implementing them every way imaginable and profiling?\n",
      "\n",
      "Here's the concrete case I'm dealing with. I have a Nx9x9 Tensorflow tensor (representing information about a series of Sudoku boards). I want to average elements along consecutive triples of elements, like so:\n",
      "\n",
      "aaabbbccc\n",
      "\n",
      "dddeeefff\n",
      "\n",
      "...\n",
      "\n",
      "I can think of 3 ways of reducing from 9x9 to 3x9:\n",
      "\n",
      "\\* convolution with a 3x1 kernel with weights (0.33, 0.33, 0.33) and stride 3\n",
      "\n",
      "\\* reshape to 9x3x3 + reduce\\_mean on axis 2\n",
      "\n",
      "\\*image rescaling ops (maybe?)\n",
      "\n",
      "But as noted, I have no intuition for which is the 'right' approach here, and (more importantly) I want to be able to move from trial and error to reasoning about performance. I'd be grateful for any advice.\n",
      "Please advise the most appropriate community or thread where I can post our request for volunteer developers ? (to help with open source project)\n",
      "\n",
      "Sky Hub AI UAP Tracker - Development tasks\n",
      "\n",
      "We need people with experience in go lang and vue.js. We are primarily using go for our backend, vue.js frontend. If you have experience in elasticsearch, mysql, vuejs, C you will also be able to get involved.\n",
      "[Sky Hub Chat Server](https://chat.skyhub.org)\n",
      "\n",
      "Many thanks \n",
      "Paul\n",
      "I seriously need help.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "So right now ( about a few weeks ago ), I started diving into ML with really no previous experience other than some average middle school and high school math/stats, and so far that has proven more than enough. This is mainly due to me being very good with Python, so I understand a lot of the concepts. ( also quick note, I am learning Keras for now as this seemed the most simple and easy way to get into ML ) However, every day I am learning more and more that there are so many more heavily advanced concepts that require so much stuff to learn. What should I expect to learn? And what would the timeline be like? Programatically I can handle most things with some moderate review, but really, I'm not so sure I'm gonna have such an amazing time grasping the concepts and math/stats of this all. Are there any resources out there that you guys would recommend to a beginner? \n",
      "\n",
      "Thank you so much for taking the time to read this. I hope you have an amazing day. :)\n",
      "Is there any research into modifying the pose of a subject in a photo?\n",
      "\n",
      "Input would be an image containing a person, and the output would be the same image except the person has (e.g.) a 'T' pose with their arms outstretched.\n",
      "If you run K-Fold cross validation, you get K models. Would it be sensible to average hyperparameters across all models and re-train one model across all data?\n",
      "I can't immediately go into machine learning from my bachelor degree. I have goals to eventually get a masters in ML, but I want to get real world work experience first for a few years. Should I get a job as a data analyst, data engineer or software engineer. Which one would be the most suitable pathway to eventually lead to become an ML engineer?\n",
      "Should I do Practical Deep Learning for Coders by Fast.ai or CS231n first? \n",
      "\n",
      "I'll probably do both but what order should I start to maximize my learning capacity to this courses.\n",
      "\n",
      "I'm done with Machine Learning by Andrew Ng (with the assistance of StatQuest and 3Blue1Brown, they really helped me sharpen my intuition as a newbie when it comes to Machine Learning.)\n",
      "\n",
      "Also someone suggested me to do Code-First Introduction to Natural Language Processing by Fast.ai and CS224n too, so I'll probably do that in the future. I hope I'm in the right track.\n",
      "I've just started learning how to make linear regression models and have a doubt related to it.\n",
      "\n",
      "The tutorial I found tells to use all the variables for the first attempt at making the model and then check the significance of the variables to filter out the ones which aren't significant.\n",
      "\n",
      "What I want to know is while including all variables, assuming we're analysing sales of different branches of a company, whether the branch number should be considered or not? And if it should be considered, should it be taken as a factor or a numerical value?\n",
      "Is there a place to discuss serious collaborations on computer vision research? Like mentorship from more senior researches for graduate or undergraduate students that would like to partner or expand their circle of collaborators?  \n",
      "\n",
      "\n",
      "I'm not even sure if that even makes sense, but as a MSc student with almost no one to discuss (my advisor actually specializes in another sub-field), I would love to collaborate with people from other countries, even if for simple discussion on machine and deep learning, and computer vision research and applications, to actual research-track projects.\n",
      "**TL;DR** Why are language modeling pre-training objectives considered unsupervised when we technically have ground-truth answers?\n",
      "\n",
      "Maybe this is stemming from my not-so-great grasp of supervised vs. unsupervised learning, but my understanding is that if we have access to ground-truth labels then it's supervised learning and if not then it's unsupervised.\n",
      "\n",
      "I'll take the masked language modeling (MLM) that BERT ([Devlin et al., 2019](https://www.aclweb.org/anthology/N19-1423/)) and many other subsequent language models use.\n",
      "\n",
      "According to the original paper:\n",
      "\n",
      "> ...we simply mask some percentage of the input tokens at random, and then predict those masked tokens... In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.\n",
      "\n",
      "If we just replace a certain percentage of tokens with `[MASK]` randomly, don't we technically have access to the ground-truth labels (i.e., the original unmasked tokens)? Shouldn't this be considered supervised learning?\n",
      "\n",
      "My argument is analogous for the next sentence prediction (NSP) task.\n",
      "What happens if you train a model with a particular batch shape (x,y,z,c), But when you build the model again, you use a different input batch shape? Does this affect the predictions of the model?\n",
      "If I hope to be involved in the field of robotics eventually, would it be best to start learning reinforcement learning? Is this the most relevant method of ML used for robotics? I’m a computing science student at a school that is known for reinforcement learning so I would like to complete a reinforcement learning project this summer and hopefully get involved with one of the labs at my university. Would this be a good path considering my interests in robotics?\n",
      "Hey all\n",
      "\n",
      "Hopefully this is a simple question. I'll try my best to explain what the reply I want will look like.\n",
      "\n",
      "I am building a segmented image dataset, I am working with a small team to construct the dataset.\n",
      "\n",
      "There were several examples of us miss labeling objects when compared to each others labels (e.g. there is a class for foot ball, one of our team members labels American football, the other labels European footballs. The class was originally intended only for European footballs)\n",
      "\n",
      "The way we are dealing with this issue is browsing the images one by one and visually inspecting if the labels are consistent.\n",
      "\n",
      "My question is, do you know of papers looking in to detecting outliers in segmented image data?\n",
      "\n",
      "I am expecting something similar to clustering or dimensionality reduction except applied to segmented image classes.  Let me know what you think\n",
      "\n",
      "Thank you very much for any help you can give\n",
      "Hello all, in deep learning, instead of early stopping with patience check, what if we decay the learning rate aggressively to try converge more to the minimum when we hit the patience check? Could it be a viable solution?\n",
      "Would anyone be willing to take on an apprentice or mentee? I would appreciate any help/resources/ideas as I’m a junior developer who is eager to learn and succeed. I am willing to put in whatever time to get better at machine learning\n",
      "Hello everyone, I am looking for a **web or desktop application** to create a dataset for **image segmentation**. I tried using Label Box, but nothing worked for me (the content of the exported file format is too complicated to understand, no examples). Also what is the standard format for such a dataset, how should images and masks be stored? Thanks in advance!\n",
      "When would we use a transformer encoder only (similar to BERT?), a transformer decoder only (similar to GPT?), or a transformer encoder-decoder (as proposed by Vaswani et al. in 2017)?  \n",
      "\n",
      "\n",
      "Excuse me if this is a shitty question that shows my lack of understanding of the literature behind transformers and self-attention based models but it's something that I've been wondering since Google posted their Vision Transformer. They only used the encoder part for their classification model. FB however used an encoder-decoder for their DETR.\n",
      "\n",
      "Similarly, from what I understand BERT only uses the encoder, GPT only uses the decoder section, while the original 'Attention is all you need' proposes the transformer as the model with both the encoder-decoder components. Are there any particular advantages or disadvantages, and situations where we should choose one specific component?\n",
      "Hi there, I want to find all the brands that are sponsoring a video on YouTube (or a podcast). Right now we are using fuzzy queries on description and transcripts. But that does not cover all cases. \n",
      "\n",
      "Often content creators use terms like “this video is sponsored/brought to you by ....” but not always. So we want to use nlp. \n",
      "\n",
      "Please suggest any idea/library or any direction to further investigation. Any help is appreciated.\n",
      "Hi \n",
      "I want to start with artificial intelligence. I have a little background on programming and good background on math and statistics. Where should I start and what are the best sources to start with?\n",
      "Hey everyone,\n",
      "\n",
      "I am not sure if this is simple or not, I hope that you can help me.\n",
      "\n",
      "I am trying to cluster small images, and then create a classification tool using the clustering model.\n",
      "\n",
      "The images are spectrograms of animal vocalization fragments (phonemes), they have different sizes albeit I assume that it is possible to pad the images to have the same size.\n",
      "\n",
      "The images have vertical scribbles, they look like letters or numbers but have a noisy background and the values (intensity)   may be important so I can't binarize the images.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Any ideas? \n",
      "\n",
      "I tried to use basic tools like PCA and t-sne to reduce dimensions and visualize and KNN and  DBSCAN for clustering, but it didn't go well\n",
      "\n",
      "Here's an example of the images (in black rectangles):\n",
      "\n",
      "[https://imgur.com/a/GHMuUor](https://imgur.com/a/GHMuUor)\n",
      "Anyone willing to review a resume for me? I'm an undergrad with a fair bit of experience in ML, but I got zero interviews so I'm trying to identify weak points.\n",
      "For my job I'm doing research for identifying parts of a product. i.e. getting the manufacturer + model of the spoiler, rims and mirrors on a car. We already have a big collection of product image data available that we could work with, and every product has multiple images shot from different angles. In the end we would upload one or more images of a product and it would (accurately) identify its different parts, including model and manufacturer.  \n",
      "\n",
      "\n",
      "Are there any ***existing*** software solutions out there that would be able to provide this functionality? Licensing/payment is not an issue.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Thanks!\n",
      "I need to detect and highlight spammy parts of text and links within messages, without discarding the whole message. Please help me find a model :)\n",
      "\n",
      "Can anyone point me to existing models or libraries I can use to easy train up a dataset? Most classification systems I can find are binary, that's not enough for my use case. \n",
      "\n",
      "I need to identify the location in a message where the link or spammy text occurs and snip it out while allowing the rest of the message. \n",
      "\n",
      "The use case is general spam, link, and insulting commentary filtering for a live chat platform.\n",
      "\n",
      "I have considered using binary classification and pruning matches using binary search to find the offending parts of the message, but this involves sending the same message through the model many times. A model that can directly output the location in a stream of characters would be ideal\n",
      "Does anybody know of 'Software Engineering for Machine Learning Researchers' resources? I'm okay at programming. Many people say that just by coding you'll get better, but I think many things I'm actually not even aware of. Also, I guess that many skills you'd pick up on a standard course might not be as relevant for an ML researcher. \n",
      "\n",
      "Any pointers would be much appreciated!\n",
      "This question has probably been beaten to death already but looking for 2 cents on my career path:\n",
      "\n",
      "I'm currently a Masters student and have a really nice supervisor that's hooked me up with a few opportunities to publish review papers and possibly my thesis when it's finished. My specific area is Deep Learning applied to medical imaging.\n",
      "\n",
      "My supervisor is pushing me to do a PhD which I am interested in, but the catch is that I already have a job lined up in consulting for one of the big 4. (I'm not looking for commentary on the big 4, I worked there and I liked it.)\n",
      "\n",
      "The problem I have is that the consulting doesn't really touch deep learning, but career wise its really solid and has loads of opportunities to progress through the company or move elsewhere after a few years. The PhD on the other hand could open up doors in Google, Microsoft or Sig for example, which would also be pretty cool.  I'm kinda stuck between the relatively easier role in consulting with good progression vs the highly technical PhD roles which I could find myself pigeon holed in.\n",
      "\n",
      "I'm pretty burnt out at the moment from COVID lockdowns and final year so I'm finding it hard to commit to doing another 4 years of college and not leaving college till I'm 27.\n",
      "\n",
      "Not sure if this is the best place to ask but any advice would be nice.\n",
      " Hey everyone,\n",
      "\n",
      "I have a traffic dataset (similar to KITTI) where license plates of cars are masked with white boxes due to anonymization purposes. Labels of vehicles (Bounding Box + Type) are available. I want to use this dataset to train an object detector for vehicles (on images without anonymization).\n",
      "\n",
      "If the data is used as is and fed to some OD-Network (e.g., Faster RCNN or YOLO) I fear that I will just train a white box detector.\n",
      "\n",
      "**Is there a way to ignore the white box regions during training and force the network to concentrate on the other parts of the vehicles?**\n",
      "\n",
      "The only way I can think of is replacing the white boxes by random noise but it might still just lead to an object detector that finds random noise patches in the image. Any other idea?\n",
      "Hey how can u deploy a ML model as an api(or make it direct backend using flask) which is taking data from an API , \n",
      "So basically i have created ML model with python using just KNN regression, a basic prediction of crypto currency that takes data from an API called alpha vantage . \n",
      "Now, my problem is that i know only one way of deploying a ml model that is by creating a pickle file and then uploading to wherever i want, but in this case where everyday i fetch data i will get different response so model will be trained differently, so a static pickle file won't work \n",
      "\n",
      "Please tell me how can i do this, i am not able to find relevant results online  \n",
      "thanks in advance\n",
      "Does anyone have experience with free Kaggle alternatives?\n",
      "\n",
      "Context: I don't have a decent GPU, so I've spent the past three months playing with ML inside Kaggle. It's generally very user-friendly, but I was thinking of using another platform as well so I had more hours to play with each week. The only one I've tried is Paperspace, and it seems less friendly out of the box. (In particular, on the TF2.0 image, I tried to install *jupyterlab-manager* so I could install bokeh, and it asked me to install npm and other things first.)\n",
      "I’m curious if actual ML engineers are excited about BlackBerry’s IVY platform. From BlackBerry:\n",
      "\n",
      "“BlackBerry IVY is a scalable, cloud-connected software platform that will allow automakers to provide a consistent and secure way to read vehicle sensor data, normalize it, and create actionable insights from that data - both locally in the vehicle and in the cloud. BlackBerry IVY will leverage BlackBerry QNX’s automotive software expertise and AWS’s broad portfolio of services, including IoT and machine learning.  BlackBerry IVY will run on the edge, inside a vehicle’s embedded systems, but will be managed and configured from the cloud. With support for multiple operating systems and multi-cloud deployments, automakers will have the ability to deliver new features, functionality, and experiences to customers over the lifetime of their vehicles.”\n",
      "\n",
      "Is this just mumbo jumbo or is this really an advance in machine learning/on board OS for autonomous driving? Sounds like a non expert threw in as many buzzwords as they could, so I figured I’d ask the folks who are building these sorts of things!\n",
      "\n",
      "Thanks :)\n",
      "I read an article about fake data scientist. The writing mentions that you shouldn't call yourself a data scientist if you don't have a technical degree, otherwise you would be a fake data scientist. What do you think about the article?  Here is the full link: [https://www.quora.com/Can-a-data-scientist-fake-it-until-they-make-it/answer/John-Singer-59](https://www.quora.com/Can-a-data-scientist-fake-it-until-they-make-it/answer/John-Singer-59)\n",
      "On StyleGAN face images are aligned, I assume this gives the network result some kind of boost? How should I proceed if I have a dataset with non human faces, such as trees? How should they be aligned?\n",
      "I’m super new to ML, I have some ok python experience. I want to create or work on a model to predict stock movements. I’m not looking for a get rich quickly thing and I understand that it won’t get it right some of the time, I’m also ok with losing the investment. I’m basically looking for a tool that can identify some swing trades entry and exit points (I have no problem if the swing duration is days to months, I’m not looking to get rich, I am looking for the experience)\n",
      "\n",
      "My question: is this something that ML can accomplish with acceptable results? Or is this just a waste of time?\n",
      "i need a little helo with neural network with handwritten numbers\n",
      "\n",
      "Hello, i just start learning deep learning, and machine learning, but its a little hard to me, for understand python, and this, and i have a test to make an neural network with  handwritten numbers.\n",
      "\n",
      "This is the code i have for this.\n",
      "\n",
      "\\######################################################################################\n",
      "\n",
      "import tensorflow as tf  \n",
      "from tensorflow.keras.utils import to\\_categorical  \n",
      "(x\\_train, y\\_train), \\_ = tf.keras.datasets.mnist.load\\_data()  \n",
      "\n",
      "\n",
      "import numpy as np  \n",
      "import matplotlib.pyplot as plt  \n",
      "fig = plt.figure(figsize=(25, 4))  \n",
      "for idx in np.arange(20):  \n",
      "   ax = fig.add\\_subplot(2, 20/2, idx+1, xticks=\\[\\], yticks=\\[\\])  \n",
      "   ax.imshow(x\\_train\\[idx\\], cmap=plt.cm.binary)  \n",
      "   ax.set\\_title(str(y\\_train\\[idx\\]))  \n",
      "\n",
      "\n",
      "x\\_train = x\\_train.reshape(60000, 784).astype('float32')/255  \n",
      "y\\_train = to\\_categorical(y\\_train, num\\_classes=10)  \n",
      "\n",
      "\n",
      "model = tf.keras.Sequential()  \n",
      "model.add(tf.keras.layers.Dense(10,activation='sigmoid', input\\_shape=(784,)))  \n",
      "model.add(tf.keras.layers.Dense(10,activation='softmax'))  \n",
      "model.compile(loss=\"categorical\\_crossentropy\", optimizer=\"sgd\", metrics = \\['accuracy'\\])  \n",
      "model.fit(x\\_train, y\\_train, epochs=10, verbose=0)  \n",
      "\\_, (x\\_test\\_, y\\_test\\_)= tf.keras.datasets.mnist.load\\_data()  \n",
      "x\\_test = x\\_test\\_.reshape(10000, 784).astype('float32')/255  \n",
      "y\\_test = to\\_categorical(y\\_test\\_, num\\_classes=10)  \n",
      "test\\_loss, test\\_acc = model.evaluate(x\\_test, y\\_test)  \n",
      "print('Test accuracy:', test\\_acc)  \n",
      "image = 7  \n",
      "\\_ = plt.imshow(x\\_test\\_\\[image\\], cmap=plt.cm.binary)  \n",
      "import numpy as np  \n",
      "prediction = model.predict(x\\_test)  \n",
      "print(\"Model prediction: \", np.argmax(prediction\\[image\\]))\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "the only issue i have its i dont know how to add a neural network for this code, cand someone could help me with that?\n",
      "Anyone used NVidia Tesla K20x? I can get it super cheap, has nice FP64 performance, and I just want to know if it will be any better than Quadro 4000, GeForce GT 710?\n",
      "Is there a way to craete a confusion matrix but instead of vales (number of correctly/ not correctly predicted images) I want to put the actual images? like this matrix:  https://imgur.com/gallery/jOhWCzp\n",
      "I have a simple CNN model built with the help of keras library (not the one from tensorflow).I am able to train the model and check its accuracy against an already existing dataset. The weighted network has been created and stored in .h5 file. Now I would like to create an application (preferably using JS) which takes an input image and I give the output as the predicted result according to my model. Could you suggest any ways on how to do this?\n",
      "Hey i'm new to ML and have few questions.\n",
      "\n",
      "1. Is ML bad for my laptop, it's using Ryzen 7 4700U and Radeon Graphics, my laptop has a really thin bodies and doesn't stand against heat for a long time. Do i had to run my ML model locally, or should i not ?\n",
      "2. For cloud alternatives, i have used Google Colab and Kaggle kernel, are they good enough for you guys to do your thing, or is it too slow for real ML engineer ? What cloud services do you use for training ML ? (Especially the free one, for learning and competitions)\n",
      "Greetings fellow ML enthusiasts, I used to run my deep learning framework on a ubuntu PC, but now I want to \"spread the workload\" by running my model on my Windows 10 Laptop with GTX 1050ti 4 GB. My question is, can anyone please suggest a comprehensive and updated guide to set up a Deep Learning environment on Windows 10? I know there are plenty of guides out there, but I want to know if there is a specific guide that is \"well-known\" and \"endorsed\" by the ML community. Thank you in advance. Cheers.\n",
      "\n",
      "http://faculty.marshall.usc.edu/gareth-james/\n",
      "\n",
      "https://www.coursera.org/specializations/statistics\n",
      "\n",
      "I did my initial research for finding a statistical lecture with mix of R and so far I heard good things about this 2. Just want to know if whom should I prioritized first. Or is there any other else better than these 2?\n",
      "\n",
      "Thank you!\n",
      "Is there any \"ML\" way to predict a binary list? For example, given 100 binary inputs, predict the next 10. I could easily do it from a probabilistic perspective using Bernoulli, but I haven't been able to find a proper MachineLearning way to do so.\n",
      " \n",
      "\n",
      "Hi everyone. For a project I'm working on, I'm trying to train a model. The model takes 12 inputs and should output based on the number of classes(I'm starting with two classes). The 12 inputs represent different aspects that determine the position and gesture of my hand, and the two classes are two gestures(thumbs up and high five).\n",
      "\n",
      "Right now, my model just looks like this\n",
      "\n",
      "`12(Input) -> 7(Dense) -> 3(Dense) -> 2(Dense)`\n",
      "\n",
      "This model seems like it would work(although I'm really just a beginner at machine learning so correct me if it doesn't make sense) but the main problem is the lack of data. After spending some time gathering data, I ended up with 50 data samples for each class or 100 data samples in total. I know this is not near enough to train effectively. Right now, I can just get more data, but in the future, I want to be able to create a model on-the-fly using only 100 data points.\n",
      "\n",
      "How can I achieve this?\n",
      "\n",
      "tl;dr: I need to train the model above with a minimal amount of data, what are ways to do so?\n",
      "I don't know if this is suited to this subreddit - as its not true ML, so if you know of somewhere else for me to look, let me know.\n",
      "\n",
      "I have a multi-dimensional optimization task where I have access to a high performance computing ring to run individual tasks in parallel as well as to run them on my local machine. I have implemented/used a few optimization algorithms like MCS and Nelder-Mead that run iterations in series trying to minimize or maximize a cost function.\n",
      "\n",
      "I'm trying to find any reading or advice on what kind of methods can leverage parallel computation to reduce runtime for a task like this, especially for tasks where single iterations can take ~5minutes or longer. If anyone know where to ask this question if its better suited to another subreddit, let me know as well.\n",
      "\n",
      "It is important to note that this is Derivative Free optimization since I have no information about the cost function, other than its values where I evaluate them.\n",
      "Does anyone know a good library for few-shot learning? I'm not too good at ML and don't want to focus too much on it as it isn't the main part of my project. I looked at Reptile(OpenAI) but all of it went over my head and I couldn't tailor the example code to fit my case.\n",
      "Hi im currently taking DataCamp course on ML,but I don't feel that it will be enough.Can anyone recommend a book to learn machine learning while undertaking those courses?\n",
      "Is Rust worth learning to do ML?\n",
      "\n",
      "I'm a complete beginner to ML, but I'm familiar with Python and have already some real experience with programming (mainly in mobile development). I want to start learning ML, and I thought that it would be a nice pretext for learning Rust too, as it seems to have already some kind of environment to accomplish the basic tasks.\n",
      "Can someone explain backpropagation in context of neural network. I've been trying to find a good explanation, but for some reason, the explanation is not clicking with me. Could someone offer a \"good, clear, concise explanation\" of backpropagation \"for dummies\" edition\n",
      "Help finding a particular paper.\n",
      "\n",
      "The network is fed images rendered from a player's perspective within a maze, such that the camera rotates 360deg for a full view of the surroundings.\n",
      "\n",
      "As this happens, we see a heatmap superimposed on a top-down image of the maze. The \"hot\" regions eventually converge into a single point which is the network's guess for the player's position.\n",
      "\n",
      "Iirc, the authors ran it on rendered mazes and real-life environments\n",
      "Just started dealing with medical images, and the images are in series.\n",
      "\n",
      "I searched some 3D networks and found lots of them designed for time-sequence data not image series.\n",
      "\n",
      "So i wonder can I just replace all operations from 2D networks to 3D (convs, pooling,etc), or is there something else different in '3D' from 2D single images?\n",
      "I am using the following code to try to run [StyleGAN](https://github.com/NVlabs/stylegan) on Google Colab: [https://github.com/jeffheaton/t81\\_558\\_deep\\_learning/blob/master/t81\\_558\\_class\\_07\\_3\\_style\\_gan.ipynb](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_3_style_gan.ipynb)\n",
      "\n",
      "and running into the error \"AttributeError: module 'dnnlib' has no attribute 'SubmitConfig\" after running the code: sc = dnnlib.SubmitConfig()\n",
      "\n",
      "Since this is coming from the dnnlib, which was written by the model's authors, not me, I'm not sure how to proceed. Any help would be greatly appreciated! :)\n",
      "How does hyper parameter needs change when we increase the model size? For example if we change the architecture from resnet50 to resnet152. Is there any trend that normally works like increase model size and increase lr or  weight decay or something? Thanks.\n",
      "What's the difference between nn.Embedding (pytorch) and vq-vae's codebook?  Are these two concepts the same idea? or does it make sense to implement codebook with the nn.Embedding class (pytorch)? I know nn.Embedding is used for a trainable lookup table. And codebook (in vq-vae) seems with the same idea to me (trainable, also a lookup table) so what's the difference? I can not clearly distinguish them. Thanks.\n",
      "I'm trying to use pre-trained models from Intel to do simple detection of objects passing by in the street. I've downloaded pretrained models from [https://docs.openvinotoolkit.org/latest/omz\\_models\\_intel\\_index.html](https://docs.openvinotoolkit.org/latest/omz_models_intel_index.html) . At this point I have an XML and a BIN-file with the model I would like to use. What I would like to do is to apply the model on a picture using a python program, but I am at a loss for how to import the model. Any ideas or guides on how to do this?\n",
      "Has there been any study on how to choose the relative weights for compatible objectives?  Especially concerning the effect this has on overfitting...\n",
      "**Would DeepMind Control Suite be enough for a publication?**\n",
      "\n",
      "It's rough as a PhD student in deep RL. Lots of industrial competition and ever-changing expectation/standards. So forgive the very general and informal question, but would the DeepMind Control Suite satisfy reviewers for a modern RL publication?\n",
      "\n",
      "Say, for a new general-purpose algorithm that achieves better performance than SAC, PPO, and A2C.\n",
      "\n",
      "Or if that’s too simple, which additional experiments (algorithms and environments) would be more ideal to test? There was a time not long ago when Atari was enough.\n",
      "Are energy based models primarily used to generate images, or can they be used for tabular data? For tabular data, how might it work differently from SMOTE?\n",
      "Hello everyone - this might be a very simple question so please feel free to point me in the right direction. \n",
      "\n",
      "I am a data science student who works as a business analyst for an insurance company. As a result, I use Excel in my day to day and R/Python at school.\n",
      "\n",
      " I was working on a fun project (solving a silly riddle) and used the Solver function in Excel to find my answer. I formatted my problem so that I effectively needed to find the optimal string of 27 values, where each value was a 1, 2 or 3. Despite the large number of permutations, Excel was able to find me the correct answer after testing some 5k combinations and did so rather quickly. \n",
      "\n",
      "I tried to do the same in R and found it was significantly more challenging, and when I did find the correct result, it took longer then in Excel. I was curious if I was using the wrong package or was otherwise confused.I set up the problem in the same way, then used a Genetic Algorithm package \"GA\" to find my solution. I was not able to find an algorithm that allowed for restricting inputs to integers, so I had to use the binary feature in this package and convert the binary string to my series of integers. \n",
      "\n",
      "This approach found a correct answer in only 32 attempts - but the run time was longer then in Excel. \n",
      "\n",
      "I guess my questions are (1) am I understanding the use of the R package correctly? Is there a better approach in R? (2) I was able to read about how GA works, however, I am vague on how this is different than the Excel evolutionary solver approach.\n",
      "I'm working with StyleGAN-encoder, using the code from this [jupyter notebook](https://github.com/Puzer/stylegan-encoder/blob/master/Play_with_latent_directions.ipynb).\n",
      "\n",
      "generator = Generator(Gs\\_network, batch\\_size=1, randomize\\_noise=False)\n",
      "\n",
      "yields the following error:\n",
      "\n",
      "[/tensorflow-1.15.2/python3.6/tensorflow\\_core/python/framework/ops.py](https://localhost:8080/#) in \\_as\\_graph\\_element\\_locked(self, obj, allow\\_tensor, allow\\_operation)    **3647**           raise KeyError(\"The name %s refers to a Tensor which does not \"    **3648** \"exist. The operation, %s, does not exist in the \" -> 3649                          \"graph.\" % (repr(name), repr(op\\_name)))    **3650** try:    **3651** return op.outputs\\[out\\_n\\]  KeyError: \"The name 'G\\_synthesis\\_1/\\_Run/concat:0' refers to a Tensor which does not exist. The operation, 'G\\_synthesis\\_1/\\_Run/concat', does not exist in the graph.\"\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Can anyone help me? thank you so much\n",
      "When you are adjusting the learning rate automatically, wouldn't you want the LR to be a decreasing function of the # of epochs? I'm working through Moroney's Tensorflow in Practice specialization and the function they use is \n",
      "\n",
      "`lr  = 1e-8 * 10 **(epoch / 20)`\n",
      "\n",
      "which is monotonically increasing as a function of the epoch\n",
      "Hey, If anyone experienced in time series can help it would be great.\n",
      "\n",
      "I am working on agriculture dataset, that contains 3 years state-wise data of agriculture farming on different crops, for ex  \n",
      "City  - 2016,         2017,        2018     crops  \n",
      "tokyo -5000ha , 2500ha,    3421ha  rice  \n",
      "Saitama -  5131ha, 3232ha, 2932ha  rice  \n",
      "tokyo - 2012ha,  312ha,    2479ha, peanuts  \n",
      "saitama - 213 ha, 892ha,  9832ha ,peanuts  \n",
      "\n",
      "\n",
      "I hope you get the idea, so How can I use time series to predict how it will be going, there are many features and I don't know how to go  \n",
      "As the data is private, I can't share the data, sorry for that!\n",
      "\n",
      "Thanks for help\n",
      "Hey everyone,\n",
      "\n",
      "I'm trying to use this loss function: [Weighted Hausdorff Loss](https://github.com/danielenricocahall/Keras-Weighted-Hausdorff-Distance-Loss) with this implementation of [U-Net](https://keras.io/examples/vision/oxford_pets_image_segmentation/#perpare-unet-xceptionstyle-model) .\n",
      "\n",
      "The loss seems to be written for TF 1.15, I'm currently using TF 2.4.0. The problem is, that I'm getting **no gradients** with this loss, when using it in training, but when I give it sample data, just evaluating the loss (forward pass) works flawless, so every function used should work with TF 2.4 or am I missing something here?\n",
      "\n",
      "Thanks in advance.\n",
      "Hello Everyone,\n",
      "\n",
      "I was wondering if someone can teach me more about Sammon Mapping?\n",
      "\n",
      "I understand it is a dimensional reduction technique, but is it in itself a clustering algorithm that can be considered an unsupervised learning method?\n",
      "\n",
      "Any insight into this mapping algorithm and how it relates to unsupervised learning would be very helpful.\n",
      "Hi everyone!\n",
      "\n",
      "I am an Austrian student, and I am looking for a real-world practitioner working with algorithms e.g., in social media or any other areas. Since it is very short-term and I have no idea where to find experts like you, I am posting here.\n",
      "\n",
      "I have to do a short interview on the subject of algorithms for an university course. The starting point was the article \"Human decisions and machine predictions\" from Kleinberg et al. \\[[https://academic.oup.com/qje/article/133/1/237/4095198\\]](https://academic.oup.com/qje/article/133/1/237/4095198]) (does not have to be read, I am preparing a short summary)The interview consists of \\~11 questions can be answered by e-mail. (It's not too specific and rather superficial)\n",
      "\n",
      "Thank you in advance for your time!\n",
      "Hi. \n",
      "\n",
      "Need to clear something up on scikit learn:\n",
      "\n",
      "Am I understanding correctly in that the `sklearn.model_selection.permutation_test_score` function performs the  y-randomization (y-scrambling) test of an ML model?\n",
      "Hey guys , happy new year ,\n",
      "And quick question , I learnt python and used it for some time , but I am facing difficulty when I have to use it in real time like mainly logical part of code\n",
      "\n",
      "Example - scrap an website using selenium.\n",
      "\n",
      "I can write the code where it goes to the page and does all selections and but when it comes to logical part then I am struck.\n",
      "\n",
      "Any suggestions and I want to apply python for machine learning , so any suggestions , how to overcome this ??????\n",
      "I wanted to know how to bring in kernel transformation to the K nearest neighbours algorithm. Any help is welcomed.\n",
      "I have implemented a transformer following Peter Bloem's blog that works well, but I am somewhat confused by the position embeddings. The material I am finding on position embeddings describes the embeddings as being with respect to a single sentence. But in Bloem's transformer model, the position embedding size is \\`(size\\_of\\_word\\_embedding, length\\_of\\_longest\\_sequence)\\` where the longest sequence has a length of 512, which surely includes multiple sentences. So the position embeddings here seem to be with respect to the entire sequence of sentences, not a single sentence. Accordingly, position 354 could be the first word of the sentence for one sample and the last word of the sentence for another sample. \n",
      "\n",
      "Am I interpreting this correctly? If so, the position embeddings seem meaningless to me. Knowing that \"cat\" is the 2nd word of a sentence vs the last word of the sentence seems meaningful. But there is no reason to believe that \"cat\" being at position 354 in sample a and also position 354 in sample b has any special meaning. How do we get a meaningful embedding when the position can mean anything?\n",
      "\n",
      "Or perhaps the usefulness of the position embedding is more within sample. So if we have a period at position 352 and \"cat\" shows up at position 354, that is different than \"cat\" being at position 353 and a period being at position 354. But then how do we learn an embedding that would be useful for the same position for other samples?\n",
      "Hi. I have been looking for articles that describe how I can expect the loss to vary when training a neural network. Obviously LR is very important. But can one e.g. say anything general about expected loss fluctuations when using decaying cyclical LR? Can one e.g. expect fluctuations in loss to only become smaller as LR cycles (and LR)  becomes smaller or is it completely problem dependent? My intuition is that loss fluctuations should become smaller as the max LR in a learning rate cycle becomes smaller. But I might be completely wrong? Does anybody know of good articles/papers that describe this subject?\n",
      "Are there any gift codes for Colab Pro? Want to buy Pro for a friend as a gift but can't find how to do it\n",
      "I am interested in learning numerical optimization from practical point of view. Is there any course (PhD level) with videos + lecture notes + solutions to assignments online?  \n",
      "I really liked  Ryan Tibshirani's course ([https://www.stat.cmu.edu/\\~ryantibs/convexopt-F15/](https://www.stat.cmu.edu/~ryantibs/convexopt-F15/)) but the solutions to assignment is not available.\n",
      "i want to start studying deep learning and neural networks. is knowing the math under the hood important, or simply beneficial? if needed, what is the correct math to learn, and how might one go about learning it with the topic of deep learning in mind?\n",
      "Does using a GPU help speed up neural network prediction (as opposed to fitting)?\n",
      "its possible to make a face generator with a pix2pix(encoder-decoder) model? what kind of input data to train I need?, gaussian nosie or the same face as the output?, a discriminative network + pix2pix model= Generative Adversarial Network?\n",
      "Hey everyone! There's an interesting problem I am facing with some regression data, any input appreciated!\n",
      "\n",
      "Assume that I am having a bunch of data points, and a regression scenario, meaning that I would like to predict a specific value of a test data point. However, the training data points I am given contain not only specific values for each data point, but also inequalities , e.g < 20 if we know that a data point has value y less than 20.\n",
      "\n",
      "The first obvious solution to that is to discard those points completely. Another solution would be maybe to adjust the loss function --let's assume MSE here for convenience -- so that, during training, if the model, for the specific data point mentioned above, predicts any value less than 20, the data point does not contribute in terms of loss. However, I would argue that this kind of an approach could harm the overall accuracy of the model, since any value below 20 will do (and many weight configurations can theoretically give minimum loss).\n",
      "\n",
      "Do you know any approach that is more robust to inequality data existing in a regression problem? Thanks in advance!\n",
      "Hi all, how would you do few-shot learning for text classification using transformers? Thanks!\n",
      "Can anyone suggest some machine learning paper they might have read recently? I am supposed to make an application utilizing the paper(in the field of cognitive computing) but I am having a hard time choosing a paper. So can anyone send any of their suggestions or an interesting paper that they have seen or you could make a cool project out of it?\n",
      "i am firmware/ bios developer but i m interested to get into data analysis with AI.  Can someone point me some paths or what i should start study/learning in order to go down this path?\n",
      "Hey fellas. I'm going to be working on some natural language processing classification models.\n",
      "\n",
      "What's the best way to deal with noise in my sample space? I'm going to be grabbing messages from chat rooms (sometimes the log will have over 100,000 comments), and moderators won't be able to catch every bannable offense. My interest is in classifying when a user is predicted to be banned.\n",
      "\n",
      "Also not sure if I should go for classification or regression\n",
      "Hi guys, I'm new to this and would like some advice.\n",
      "\n",
      "I'm trying to train a model to identify printing errors with two inputs. The first input will be a picture of the \"sample\" print, and the second one will be a picture of the \"new\" print. The program will tell if there are printing errors in the \"new\" print compared to the \"sample\" print.\n",
      "\n",
      "Is this something that a beginner will be able to do? If so please point out some resources that I can look into.\n",
      "\n",
      "Thanks a lot!\n",
      "Any reason why my accuracy is so low? I'm getting 10% on cifar10. If I make the relevant changes to the shape and and input size and apply it to mnist I get > 90%. I'm new to this but not sure why the results are so drastic between the 2 datasets.\n",
      "\n",
      "    from keras.datasets import cifar10, mnist\n",
      "    from keras.models import Sequential\n",
      "    from keras.layers.core import Dense, Activation\n",
      "    from keras.utils import np_utils\n",
      "    from tensorflow import keras\n",
      "    \n",
      "    (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
      "    \n",
      "    X_train = X_train.reshape(50000, 3072)\n",
      "    X_test = X_test.reshape(10000, 3072)\n",
      "    \n",
      "    classes = 10\n",
      "    Y_train = np_utils.to_categorical(Y_train, classes)\n",
      "    Y_test = np_utils.to_categorical(Y_test, classes)\n",
      "    \n",
      "    #Set hyperparameters\n",
      "    input_size = 3072\n",
      "    batch_size = 100\n",
      "    hidden_neurons = 100\n",
      "    epochs = 25\n",
      "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
      "    \n",
      "    #\n",
      "    model = Sequential()\n",
      "    model.add(Dense(3000, input_dim=input_size))\n",
      "    model.add(Activation('sigmoid'))\n",
      "    model.add(Dense(2000, input_dim=3000))\n",
      "    model.add(Activation('sigmoid'))\n",
      "    model.add(Dense(classes, input_dim=2000))\n",
      "    model.add(Activation('softmax'))\n",
      "    \n",
      "    #\n",
      "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
      "    \n",
      "    #\n",
      "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=1)\n",
      "    \n",
      "    #\n",
      "    score = model.evaluate(X_test, Y_test, verbose=1)\n",
      "    print('Test accuracy:', score[1])\n",
      "\n",
      "Any help is appreciated. Thank you.\n",
      "Hello ML Enthusiasts,  \n",
      "I have worked with javascript in the past but I have always been fascinated by ML. I never really had the guts to learn ML but now I want to do it. So as an absolute beginner how should I start learning ML. Right now I don't have any idea of how ML functions or what are the concepts that I should learn. I just don't know where to start.  \n",
      "\n",
      "\n",
      "I googled about how to start learning ML, but the opinion varies on learning by joining video classes or learning by books. As a beginner, I do not want to spend much money into this. So are there any free video courses that I could start watching or maybe free e-books. I thought the best place to ask was right here, where I would get an honest opinion from someone who has already dove deep enough in the ML world.  \n",
      "\n",
      "\n",
      "Thanks in advance. Any advise or opinions are welcomed.  \n",
      "\n",
      "\n",
      "Maybe after few years, I hope even I could answer the question regarding ML in this community :)\n",
      "I have a problem, could anyone offer me advice?\n",
      "\n",
      "The book Eikon Basilike has no known author, it could have been written by either Charles 1st, John Gauden or someone unkown. We have work written by Charles 1st (The Kings cabinet opened) and work written by John Gauden (A Religious and Loyal Protestation).\n",
      "\n",
      "How possible would it be to confirm who wrote the book Eikon Basilike?\n",
      "Hello! I am trying to run [stylegan-encoder](https://github.com/Puzer/stylegan-encoder) using their pre-written [jupyter notebook.](https://github.com/Puzer/stylegan-encoder/blob/master/Play_with_latent_directions.ipynb) I am using tensorflow 1.14 and have ensured that it is connected to a GPU on google colab.\n",
      "\n",
      "I have gotten the full notebook running but when I try to learn new vectors in the terminal, I am getting an error coming from the authors' encode\\_images.py\n",
      "\n",
      "**I am running in terminal:** python encode\\_images.py aligned\\_images/ generated\\_images/ latent\\_representations/\n",
      "\n",
      "**And getting this error:**  \n",
      "File \"/content/stylegan-encoder/encode\\_images.py\", line 73  \n",
      "img.save(os.path.join(args.generated\\_images\\_dir, f'{img\\_name}.png'), 'PNG')\n",
      "\n",
      "I have read that f'{} requires python3, but I cannot run python3 without tensorflow 2.x, while this model requires Tensorflow 1.14. I am not sure how to get past this.\n",
      "\n",
      "Can anyone please help me? Thank you!\n",
      "Hello! I am trying to find a face recognition algorithm in order to organize an image archive. Most of the photos are in low resolution. What do you suggest for this task? Thanks in advance!\n",
      "Hi, Im new to ML (literally just finished CS50 AI) and Im trying to build a Cat vs Dog Classifier with Tensorflow and Keras\n",
      "\n",
      "However, my validation loss and accuracy is very unstable and fluctuates between each epoch. Also, my validation accuracy seems to plateau very early on whereas my training accuracy continues to increase, resulting in a rather significant discrepancy between my training and validation metrics. Ive tried using Dropout between layers to reduce overfitting but my accuracy seem to cap at 85% for all the tests I've run.\n",
      "\n",
      "Accuracy: [https://imgur.com/gc1w835](https://imgur.com/gc1w835)\n",
      "\n",
      "Loss:  [https://imgur.com/BfWsXV9](https://imgur.com/BfWsXV9)\n",
      "\n",
      "My sample size is 13,000 for each class, using 30% for testing. My images are 100x100px, Grayscale, normalised to pixel values between 0 to 1.\n",
      "\n",
      "Below is my model architecture:\n",
      "\n",
      "[https://pastebin.com/bpgt66Gh](https://pastebin.com/bpgt66Gh)\n",
      "\n",
      "Is there any other way to improve my model? Thank you!\n",
      "Hi looking for some tips for an idea. Is it possible to make an ai picture search for pdf? For example, if I had a book about car engines and wanted to look for a part. I could type a part number and the search would take me to a picture of that part?\n",
      "\n",
      "Edit: please share if this has already been done thanks!\n",
      "Hi, I am looking for guides or books with info about **satellite imagery** segmentation and classification (**Tensorflow** is preferred). So far I found DeepSat V1 and DeepSat V2, but with my low exp it's not enough. \n",
      "\n",
      "Thanks for any resources!\n",
      "Hello. I know very little about ML, but was recently tasked with investigating the idea of using machine learning in AWS to predict how successful a production code release will be based on previous code releases. \n",
      "\n",
      "I'm looking to create a VERY simple proof of concept showing that this can be done. Any pointers of tools to use, methods to consider, pitfalls to avoid, etc. would be greatly appreciated! Thank you in advance\n",
      "Hello all,\n",
      "\n",
      "I am researching the problem of predicting monthly energy consumption and plan to compare SVR, CNN-LSTM hybrid, and RF's performance on it.\n",
      "\n",
      "The available data contains 4000 individual houses and is split into 5 datasets:\n",
      "\n",
      "* Time-series historical energy usage data\n",
      "* Time-series weather data (separate dataset for average, min and max)\n",
      "* Information on household and its occupants\n",
      "\n",
      "I've done a lot of machine learning projects before but have never used multiple different datasets before for a singular purpose.\n",
      "\n",
      "How would you utilise these multiple datasets in order to learn from them?\n",
      "\n",
      "My current thinking is to apply some unsupervised learning such as k-means to categorise each instance by household and climate type and then train a separate energy prediction model for each.\n",
      "\n",
      "Wondering what others' would do to approach this problem and if there is a better way to do this than the way I have proposed?\n",
      "Tensorboard Images are shown incredibly overcontrasted - is my network learning with the wrong colors?\n",
      "\n",
      "I am trying to get a training done with the tensorflow object detection branch:\n",
      "\n",
      "[https://github.com/tensorflow/models](https://github.com/tensorflow/models)\n",
      "\n",
      "following the instructions from this tutorial.\n",
      "\n",
      "[https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)\n",
      "\n",
      "Everything seems to be working fine and after labeling and creating tf records I am finally training my network (SSD-Resnet50 on 640x640). After training, however, the quality of detections is rather poor, although I am (for now) only training with 1 class, I have sufficient images (750) and training in tensorboard looked as usual. However, when I switch to the Images-Tab in tensorboard and preview the images from the current batch, they are always contrasted too high, such that nothing is recognizable anymore.\n",
      "\n",
      "I was trying to find an answer if it might be preprocessing (which I dont, I am only using vertical and horizontal flip), but couldn't find anything.\n",
      "\n",
      "I thought, that maybe tensorflow is not able to read the images correctly (I have had my fair share of training with bgr-images from open-cv and then testing with rgb-images from PIL, wondering why the results were okish, but never good), but since I am using tfrecords and standard jpg imgaes, I would rule this problem out, aswell.\n",
      "\n",
      "Another suggestion I found was that it is a Tensorboard problem and that maybe updating would help, since I was using Tensorflow 2.2.0 (from the Tutorial), but also Tensorboard 2.4.0 didn't change the problem.\n",
      "\n",
      "At this point, I am out of solutions and would like to know, if there is anyone else that has (had) the same problem and maybe knows, how to fix it?\n",
      "\n",
      "Also: if my network is training with these corrupted images, the quality of detections can't be that good, which would further motivate me to get rid of this problem.\n",
      "Hello.  \n",
      "I am trying to see, if a problem could be solved with ML. The problem could be simplified that way:\n",
      "\n",
      ">Imagine a variant of scrabble where certain letters have special rules (eg. at the beginning of the game, you draw a letter and you can reuse that letter every turn). Given a set of letters (and their specific kind), determine if that hand is potentially high-scoring.\n",
      "\n",
      "From what I have read, a simple transformer encoder would be a good solution. Instead of position embedding, we can use \"kind embedding\". The position-independent nature of self-attention also seems appropriate.\n",
      "\n",
      "I'm curious to see, if there is anything I could have missed.\n",
      "Hi.  \n",
      "I am currently studying ML via: \"Hands-On Machine Learning  by Aurélien Géron\" and DataCamp courses, but I don't feel like I am doing any progress. Do you think  that I shoud start trying out kaggle competitions, or any other form of practice? Or just finish the book first and then try kaggle?\n",
      "Is it better to normalize all my data by the same factor or normalize each feature/column separately.\n",
      "\n",
      "Example: I am doing a stock prediction model that takes in price and volume. A stock like Apple has millions of shares traded per day while the price is in the hundreds. So normalizing my entire dataset would still make the price values incredibly small compared to volume. Does that matter? Or should I normalize each column separately?\n",
      "Is it good idea to learn machine learning move to Math major from the Informational technologies?\n",
      "Hi.\n",
      "\n",
      "I'm 21 and currently studying Mechanical Engineering. I'm planning learn ML and build my  career on it. As a Mechanical Engineering student, can I find a job related to ML after I graduate from university ?\n",
      "Does anyone have some recommended resources for data prep cookbooks on predictive churn modeling? Ideally covering use case of starting with customer logs and getting to finalized summary tables, with advice for partitioning train/validate/test, as well as preventing the same customer from appearing in multiple partitions, etc.\n",
      "I have a minor question: You see all these youtubers making AI to, for example, race cars down a track. And the AI generally gets pretty good at it. However, what rarely seems to be demonstrated is, do these AI actually learn how to race on a track, or how to most efficiently complete that one specific track?\n",
      "\n",
      "If you put them to a similar task but with a different environment, would they use their previous \"experience\" to not just bumble around for the first 100 generations?\n",
      "I am trying to understand variational autoencoders on the MNIST dataset with very little background in probability and statistics.\n",
      "\n",
      "1) I don't understand the need to marginalize over the latent variable z when computing p(x|z)p(z). In the code, p(z) is first computed by sampling from a normal distribution before it is sent through a neural network to generate the image i.e. compute p(x|z). Why don't we need to marginalize over the latent variable z here ?\n",
      "[deleted]\n",
      "This is a question I came across in a ML course I'm taking.\n",
      "\n",
      "`Suppose we generate a training set from a decision tree and then apply decision-tree learning to that training set. Is it the case that the learning algorithm will eventually return the correct tree as the training-set size goes to infinity? Why or why not?`\n",
      "\n",
      "I think that the accuracy of the new decision tree we are building will be bounded by that of the original decision tree. So it won't necessarily be the correct decision tree as it will stick to the trends it sees in the first decision tree we are pulling the training data from.(no matter how much training data we generate and present)\n",
      "\n",
      "I would like to know your idea on this.Thanks for your time.\n",
      "I'd like to have your thoughts about an approach that I'm currently trying in my job. \n",
      "\n",
      "My objective is to understand client classification and identify the most important drivers (**features**). Hence I am not interested into predicting anything for the moment.\n",
      "\n",
      "I know that the common way to do this would be with unsupervised learning. However, I would prefer to not go with this method for three reasons: \n",
      "\n",
      "* I am not a big fan of unsupervised learning as the accuracy is more difficult to measure \n",
      "* I have access to the contract type of each client so I’d like to use it as label \n",
      "* I’d like to explain my results, and for this task there are some quite useful libraries for supervised learning (SHAP, LIME) that I have experience with \n",
      "\n",
      "Since I’d like to understand patterns and **not** predict anything at the moment, I am more interested in the training phase. Let’s say I use a basic Decision Tree. My idea is to fit the model to the data and look at the most important features. \n",
      "\n",
      "Here is my question: \n",
      "\n",
      "For such a task, is it ok to evaluate a model on the training data themselves? I know this is usually quite a no-go because we want to generalize the algorithm the best as possible, but I believe here it makes sense to do so here.\n",
      "I would like to do a regression Task (find x,y position of an Object in some images). Therefore I have about 2000 annotated training images (I can annotate more maybe up to 20000). My first thought was to use CNN's but the size of my input data is not consistent and I don't want to lose feature by resizing or cropping images. What do you think? Is there a better way than using CNN's? Maybe  FCNN ?\n",
      "\n",
      "ps. Image sizes are aprox 120 x 50 pixel\n",
      "Why one-hot vectors is considered a non-sparse data type? I am not from a data science background. From my perspective, the representations of one-hot vectors contain a lot of 0's inside, thus should very well belong to the sparse data type. Please correct me if you understand why.\n",
      "Need help!!! Looking for Anyone who has experience in fast neutral style transfer\n",
      "Hi all, \n",
      "\n",
      "I'm building a DL rig for a student organization and I'm wondering **how to share it with students**. I want to be able to **create VMs and erase/reconfigure them** if students mess up. How would I go about doing that ?\n",
      "\n",
      "Thanks !\n",
      "How do you know if you're right when carrying out classifications? I just completed a Machine Learning class and achieved a pretty good grade overall despite genuinely not having a clue as to what I was doing. \n",
      "\n",
      "For example in an assignment that we had, the task was to classify the prevalence rate of diabetes within a community based on the shopping habits of the community. Some classifications which were used included KNN and Gaussian Naive Bayes and I understand what these do but what do their results actually mean? \n",
      "\n",
      "Lets take KNN. I understand that kNN classifies based on the distance between a query and its neighbours but if I obtain its accuracy I get a random percentage lets call it 'x'. What is 'x'??? Or say I obtain its F1-Measure, I get another percentage which is based on the models precision and recall but how is any of this actually useful? It really just seems like you're applying functions to datasets and then making an educated guess as to if they're correct or not. \n",
      "\n",
      "Furthermore, how do you even test if your model is right? If I take the diabetes example, do I validate my model based on other shopping datasets? And if I do this, why on earth would I trust given correlation doesn't imply causation.\n",
      "Thoughts on a necessity of a second course on Linear Algebra?  I've finished quite a detailed study of a first course, up to SVD and all applications in between (vector spaces, eigenvalues, orthogonality, similarity, inner product spaces, etc... - Something I've done in the past, but it's worth refreshing).  Is it worth continuing on?  I'm thinking my time is better spent moving into a more detailed study of probability at this point in terms of basic math.\n",
      "Looking for architecture recommendations for time series prediction. My dataset has 5 input variables and looking to predict one variable. Happy to continue discussing. Thanks :)\n",
      "Imagine a math app for elementary school students. Each math problem in the database has a tag (e.g., simple addition, long division). In answering x amount of question sa day, the ML model identifies problem areas and gives the appropriately tagged problems so that students can zero in on that particular tag/skill.\n",
      "\n",
      "What ML model would this be? A recommendation system?\n",
      "\n",
      "And is there some already existing, available code I could view to see how it would look in the education context (instead of Netflix) somewhere on GitHub?\n",
      "How does task that aren't Image-to-image translation work with Pix2pix?\n",
      "\n",
      "zi2zi, a Chinese alphabet generating [GAN](https://github.com/kaonashi-tyc/zi2zi) uses pix2pix for generating images. I also have seen many other applications using pix2pix for tasks that aren't related to image-to image translation. I compared the code of zi2zi with regular pix2pix, and found some code that I can't understand. \n",
      "\n",
      "1. What is the target source and where is the random noise? Unlike image-to-image translation tasks where there exists an obvious target image, what is supposed to be the target source for character generation? \n",
      "2. Suppose the output of the encoder portion of the unet is the latent space, then how are we supposed to set the latent space to a certain value for evaluation, exploration of the latent space while the decoder is effected by skip-connections of the encoder network?\n",
      "What is the best conditional gan that you can fine tune with your own data?\n",
      "After applying K-Means clustering on unlabelled data, why would you run a  linear classifier using as pseudo labels the cluster assignments? And What type of regularisation would you use and why?\n",
      "I think this has probably been asked before, but I tried looking on subreddit search box for \"project sources\" but I nothing came up that fit what I was searching for. I'm trying to do the Google TF Certification, and I thought it'd be best to learn by doing instead of just watching Coursera/Udacity videos and doing their exercises.\n",
      "\n",
      "Basically once upon a time, I think I had surfed the web and came across a book that resembled O'Reilly's [\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) but it was particularly for something like, \"15 Projects to start with Machine Learning\" which of course if one googles that particular keywords, there's PLENTY to choose from, but nothing like a tutorial, just ideas. Like [this](https://data-flair.training/blogs/machine-learning-project-ideas/) or [this](https://www.kdnuggets.com/2020/11/greatlearning-ai-project-ideas-beginners.html)\n",
      "\n",
      "I'm not new to Python, but still just dipping my toes on Machine Learning. I've not made nor transfer learned any models whatsoever, but I learn best when I'm confronted with a project or in particular, a problem and then try to find a way to solve it. \n",
      "\n",
      "**tl;dr:** So my question is, do you guys have used any particular source to do the kind of \"list of hobby project\" that I mentioned above and can you share the source to me? Or even better, know the book that I was talking about, and prove that I didn't daydream that book? :D Thanks!\n",
      "How do systems like DeepMind's MuZero know whether they've lost a game of Atari?\n",
      "What are the dimensionality of all the variables used in the K-means algorithm?\n",
      "I’m pretty new to ML and AI application. I’ve done a subject back at uni about the theory but this is a whole other ballpark.. I’m currently trying to make a NN with Node.js, and how do I know if the config i’ve set for a certain NN will eventually learn what I’m trying to accomplish or just fail? Thanks!\n",
      " Why are the neural networks weights initialised with random values?\n",
      " How can one decide whether the network is overfitting or underfitting the data?, What is the process that we need to follow.\n",
      "When is your dataset suitable for K-means clustering?\n",
      "So, far I have only seen that metric learning is applied to image data. It means we don't need metric learning for tabular data?\n",
      "I'm not a ML developer/researcher. I'm thinking of taking some courses later down the line, but I've got plenty of python experience and can navigate my way around existing code given enough time/effort.\n",
      "\n",
      "With that in mind, I'm currently looking for some code which can generate an artificial face, and then create a set of images of that same face from different angles and with different expressions.\n",
      "\n",
      "I'm sure there's something like this already out in the wild. Could someone point me in the right direction?\n",
      "\n",
      "Thanks!\n",
      "I have a very general question. My impression of the current state of AI is that we have found a method (neural network) that work very well with image and pretty well for text and numbers and that kind of it. if you can shoehorn your problem into an image, like go or protein folding, you can use NN to solve it. if you can't like music then your kind of out of luck.  How far from the truth that assessment is ?\n",
      "I am trying to apply a NN to create an AI for a card game. For the input layer, I have features like (1) the card in your hand, (2) the cards around you that are known via trading, (3) number of players, etc.\n",
      "\n",
      "My issue is that I want the NN to decide whether or not to pass the card in its hand, but if I make this the output of the NN, then there is no feedback (such as winning or losing the hand) indicating whether this was a good choice.\n",
      "\n",
      "After typing this, it seems that the best route may be to have the output be the machine's confidence in winning with their current card, and if that confidence is below some threshold then it will pass the card?\n",
      "\n",
      "Thoughts?\n",
      "\n",
      "(p.s. the only experience I have with machine learning is Andy Ng's course on coursera)\n",
      "I have a question about classification using neural networks. Let's take a simple example of three classes classficiation and a neural network with 3 layers (one input layer, one hidden layer and one output layer). Naturally, the output layer consists of three activation nodes, the one with the highest output result determines the classe in which the input belongs. \n",
      "\n",
      "Why is the sum of the results of the three output units isn't equal to 1 ? Naturally the result of each of these units is the probability that the input belongs to a class, and since the classes are disjoint and they \"make up\" the universe of outcomes, the probabilities must sum to 1, I'm I wrong ?\n",
      "\n",
      "I hope someone can help me understand why it's not the case. Feel free to explain in advanced mathematical terms or redirect me to website/videos... that prove why it's not the case.\n",
      "In several cases, the covariance matrix is singular and cannot be inverted. In which cases is this more likely to happen? Why is this a problem?\n",
      "Looking to get into ML.  Have a 2yr degree in programming thats done me well and have experience in python, c++ and C#.  Any good recommendations on where to start?\n",
      "What are the limitations and assumptions of the K-means algorithm?\n",
      "\n",
      "What is the time complexity of the K-Means algorithm, expressing it as a function of its inputs such as the number of clusters and/or the number of data points?\n",
      "There are no assumptions of kmeans \n",
      "Limitation : not suitable for high dimension data.\n",
      "K-means does not initialize random centroid far from each other ( K means ++ solve this problem )\n",
      "[deleted]\n",
      "Can anyone suggest me a deep learning based object detection model that is best for detecting objects on webpages?\n",
      "been trying vision transformer, can we put something (layers) before or after the transformer?? test accuracy is rubbish but train is somehow good\n",
      " I've been looking at the problem of *representation learning*, and I'm trying to reformulate the different types of learning problems to make representations appear explicitly.\n",
      "\n",
      "We can typically see the following in the literature (with *x* the input, and *y* the target/class):\n",
      "\n",
      "* Supervised Discriminative Learning: *p(y|x)*\n",
      "* Supervised Generative Learning: *p(x|y)*\n",
      "* Unsupervised Discriminative Learning: *p(g(x)|x)*\n",
      "* Unsupervised Generative Learning: *p(x)*\n",
      "\n",
      "As I was saying, I'd like to make \\*representations\\* appear explicitly in those formulations. By representations I mean the last set of features produced by a network's backbone, and that can be used for transfer to downstream tasks. Staying generic, I denote these representations *f(x)*, and as a consequence came up with the following formulations:\n",
      "\n",
      "* Supervised Discriminative Learning: *p(y|f(x))*\n",
      "* Supervised Generative Learning: *p(x, f(x)|y)*\n",
      "* Unsupervised Discriminative Learning: *p(g(x)|f(x))*\n",
      "* Unsupervised Generative Learning: *p(x, f(x))*\n",
      "\n",
      "I wonder what you think about it, because I'm not 100% convinced myself! For instance, I'm not entirely sure if *x* should still appear for the discriminative approaches (i.e. *p(y|f(x),x)* and *p(g(x)|f(x), x)* instead), as the representations already depend en *x*. Likewise, I'm not sure if the representations should be part of the joint or the condition for generative approaches (i.e. *p(x|f(x),y)* and *p(x|f(x))* instead). I could see how both could be rationalized.\n",
      "\n",
      "What do you think?\n",
      "I have been doing biomedical image segmentation on brain MRI images and basically I have been using 2D and 3D unets which is the most dominant architecture in this field.\n",
      "\n",
      "I am keen to explore other model architectures and maybe there are things people have done in the computer vision field which may be translated to biomedical image segmentation. Looking for suggestions and potential interesting things to try.\n",
      "For the MS COCO dataset format, what is the area property supposed to represent? I am under the impression that it is the area of the segmentation masks if there are any, but the framework that I am using thinks that it is for the area of the mask, and if there is no mask then it should be the area of the bounding box.\n",
      "I've been playing with VAE-variants, and am wondering if there are related works on the implication of training the encoder and the decoder networks with different optimizers, eg. Adam with lr\\_enc for the encoder and SGD with lr\\_dec for the decoder.  I'd love to hear more about other's experiences on this kind of training or related works. Thank you!\n",
      "How is \"Next Gen Stats powered by AWS\" in the NFL implemented in machine learning?\n",
      "I'm trying to train image segmentation model with transfer learning using [https://github.com/qubvel/segmentation\\_models/](https://github.com/qubvel/segmentation_models/).\n",
      "\n",
      "1. Is there anything specific to take in account for constructing single class (background + target shape) image dataset from this task? I currently have \\~250 images with 1-4 target shapes labeled in them.\n",
      "2. Should dataset include images with only background? Is class imbalance something is should worry about here?\n",
      "3. I'm currently using dice loss + binary focal loss combination. Is it overkill for just background and single target class?\n",
      "Is there a site, database or something similar where I can download .pkl pretrained models for stylegan2? I’ve looked everywhere and everything I can find is the same 10 ones in a GitHub post\n",
      "Anyone heard about admission at MILA?\n",
      "I'm working in image classification. Someone mentioned that I could try to do input recovery from a tensor formed from a given layer of the DNN. In other words, can I recover the original test image from a tensor produced at the output of a given layer?\n",
      "Im looking to get started on a simple revenue projection model broken down by days and 15 minute intervals. The inputs would be historical revenue generated, weather data, promotion data, etc.\n",
      "\n",
      "Where do you suggest I get started? I'd like to try something out small scale, free if possible as a proof of concept. Thanks.\n",
      "I have a project deadline coming up, and although my model works, idk how accurate it is\n",
      "\n",
      "i have these three numbers: `Mean Absolute Error: 3.4079000949583915 Mean Squared Error: 16.84073524874502 Root Mean Squared Error: 4.103746489336911` but I'm not sure what they mean, whether its good or bad, or how to fix it\n",
      "\n",
      "[https://colab.research.google.com/drive/1Sr\\_0zuhNzwLr-j2HuMa43xhFnz1euTHX?usp=sharing](https://colab.research.google.com/drive/1Sr_0zuhNzwLr-j2HuMa43xhFnz1euTHX?usp=sharing)\n",
      "\n",
      "thats the notebook \\^\n",
      "\n",
      "ty for any help in advance\n",
      "I am training object detection models weekly that need to do predictions on millions of images that are stored on data servers. During testing this becomes a great bottleneck and increases testing time immensely. The images are currently stored in the png format. One method I could think of was converting them to jpeg and store locally but that still would be not very efficient. Is there any hashing method or any other compression strategy that would allow for storage of the images locally to speed up the testing time?\n",
      "Hi everyone, SUPER new to this. I sometimes have a hard time learning if I don't understand the big-picture concrete ideas that go along with the practical execution of concepts, so I would love someone to guide my thinking a little bit. So this is more of a \"tell me in which ways I'm right and which ways I'm wrong\" situation.\n",
      "\n",
      "My initial understanding is that creating a Machine learning model is essentially being given (or creating) a dataframe (dataframe1). And then using the information given in that dataframe to create a new dataframe (dataframe2) with some of the data from dataframe 1, but also some new data that comes from your human understanding of the data in dataframe1 and how you can create new data that appropriately defines the relationship between two variables. Sometimes you leave out data from the original dataframe that isn't important (for example, free throw percentage might not be as important to the flow of a predicted basketball game, as it's isolated, but the amount of free throw attempts might be relevant). \n",
      "\n",
      "So my understanding is that my ability to set aside irrelevant information and create new, relevant information (based on original  data) will be the things that determine the strength and accuracy of my model.\n",
      "\n",
      "So machine learning (in practice) seems like it's essentially creating a new, more efficient collection of data based on your exploration and understanding of the data provided, Testing and Trial-and-erroring your assumptions along the way?\n",
      "\n",
      "Is that even close to being right? I know obviously there's a lot more to consider and think about and learn, but any clarification with this massive, concrete idea would really help my ability to direct my learning!\n",
      "I want to get involved in open source Machine Learning projects, i.e. building software for new algorithms, etc. Does anyone have any sources that I can look into?\n",
      "Not sure if this is the right place for this but I'll give it a shot. I'm using tensorflow and recently my convolutional networks have been running about 10 times slower than usual. I tried running older code to see if it was a change i made but that ran slowly too. Plain fully connected networks run fine. \n",
      "\n",
      "I tried contacting the university's administrator (I'm running models on a university server) and they said that they changed the version of Cuda over December from 11.0 to 11.2. Apparently the older versions are still installed so I could theoretically tell the program to use a different version, but I don't know how. Would I change it in the program or in my virtual environment or somewhere else? \n",
      "\n",
      "Also does anyone have any other ideas why my models would be running slower than usual?\n",
      "**Feature Importance in Boosting vs Bagging Methods**\n",
      "\n",
      "I am currently working to understand which of the metrics I am collecting are most informative for both regression and classification problems using Feature Importance. The datasets I am working with are small (N\\~50-200) and I am getting similar performance with Random Forest and XGBoost, but I noticed that the Feature Importance scores are very different between the two models. \n",
      "\n",
      "I am concerned that the higher weight assigned to elements that are more difficult to predict and to learners with better accuracy in boosting models will assign a higher Gini score to features that are better at predicting outliers (elements that are more difficult to predict) in the dataset. \n",
      "\n",
      "Since I am interested in the features that are informative for the majority of elements, is it better that I use a bagging method instead of boosting?\n",
      "https://trends.google.com/trends/explore?date=all&q=pytorch,tensorflow\n",
      "\n",
      "Why is the trend for PyTorch and TensorFlow going down? Are there new upcoming libraries or what is the reason? I am new to Machine Learning.\n",
      "Hey, i hope some smart people can answer this question! I'm trying to understand supervised machine learning in diagnosing exacerbations in COPD patients.  Usually the input variables or parameters are oxygen saturation, respiration rate etc (physiological parameters).\n",
      "\n",
      "How can one ensure that the predictor values / input values are correlated to the output? (exacerbations). I can't really understand how one can predict an exacerbation, without being 100% sure that for an example -> oxygen saturation is correlated with an exacerbation...\n",
      "\n",
      "Doesn't machine learning work by a human labeling the data, like \"oxygen saturation of \\*some random value\\* is labeled as an exacerbation? Or does the machine learning process figure out the value itself?\n",
      "Hi! \n",
      "I’m stumped on how to do this thing with error estimation. I feel so out of my element and no matter what I try I can’t seem to get a good estimation of how well or badly my model is predicting a specific element. \n",
      "I have a regressor model predicting a property for different elements of a population.  I was hoping I could cluster the population and have similar model errors within each cluster, and use that to estimate error for a given element of the population, but I cannot find a good clustering method for it. \n",
      "\n",
      "Are there other ways to do error estimation? I’d like to be able to report x+-y with the y value not being huge for each element (and have a different y value for each element, as appropriate) \n",
      "\n",
      "Thank you so much everyone!!!\n",
      "I have a multi-class classification problem in which one of the classes I am trying to predict is very sensitive to variations of the input data. Additionally, the input data has a very wide range of values. \n",
      "\n",
      "For simplicity we can assume I have one feature X that ranges from 0 to 500, and three classes, A, B, and C. Samples of class A are mostly located in 0 < x < 0.1, class B is mostly in 0.1 < x < 350, and class C mostly in 350< x < 500. Which ML models would do well and which would struggle with this type of problem in which class A is located in a tiny fraction of the feature space compared to B and C?\n",
      "\n",
      "Thank you in advance!\n",
      "Why does a vertical convolution pattern recognize horizontal lines?\n",
      "\n",
      "I was following the machine learning tutorial on [google codelabs](https://developers.google.com/codelabs/tensorflow-lab3-convolutions#3) and they defined a convolutional filter as :\n",
      "\n",
      "```\n",
      "filter = [\n",
      "\n",
      "    [-1, 0, 1],\n",
      "\n",
      "    [-2, 0, 2],\n",
      "\n",
      "    [-1, 0, 1]\n",
      "\n",
      "]\n",
      "```\n",
      "\n",
      "All the middle terms are zeroes so it seems as if this detects vertical edges, but the result had horizontal edges highlighted.\n",
      "\n",
      "Also it doesn't seem like they transposed it because this is how the result was calculated:\n",
      "\n",
      "```\n",
      "for x in range(1,size_x-1):\n",
      "\n",
      "  for y in range(1,size_y-1):\n",
      "\n",
      "      output_pixel = 0.0\n",
      "\n",
      "      output_pixel = output_pixel + (i[x - 1, y-1] * filter[0][0])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x, y-1] * filter[0][1])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x + 1, y-1] * filter[0][2])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x-1, y] * filter[1][0])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x, y] * filter[1][1])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x+1, y] * filter[1][2])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x-1, y+1] * filter[2][0])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x, y+1] * filter[2][1])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x+1, y+1] * filter[2][2])\n",
      "\n",
      "      output_pixel = output_pixel * weight\n",
      "\n",
      "      if(output_pixel<0):\n",
      "\n",
      "        output_pixel=0\n",
      "\n",
      "      if(output_pixel>255):\n",
      "\n",
      "        output_pixel=255\n",
      "\n",
      "      i_transformed[x, y] = output_pixel\n",
      "```\n",
      "Is studying probabilistic machine learning and/or computational statistics a good time investment?\n",
      "How do yall deal with different sampling rates in your data?\n",
      "\n",
      "One obvious solution is to resample them to be the same rate, for example, if you have 500Hz and 1kHz in your data, you could just downsample all data to 500Hz. However, when parts of your data's sampling rates have uneven multiples of each other, for example, 512Hz and 1kHz, resampling requires your own interpolation (your choice of linear, cubic, ...). This method works to some degree for me, but I'm curious whether any of you have heard about/done any work on sampling-rate-agnostic architectures? \n",
      "\n",
      "My specific use case is in 1D sensor readings, but I think work in computer vision should also be helpful.\n",
      "[text data classification question] I'm new to AI ML. I'm writing a web app for my business that automates entering bills into quickbooks in order to reduce as much of the manual keyboard entry typically required for this task. Part of the needed automation is assigning an account to each line item on a bill. Right now I look at the contents of the bill line item and make the proper account selection. Example: a line item that contains the text 'Ranger Pro' should map to a COGS account called 'Herbicide'. What should I read to help me determine best classification method?\n",
      "I'm very new to ML and my project involves using ML to find the 'optimum' point to connect a set of three coordinate points. The model will be used for extending branches of a tree-like structure and I have an initial 'tree' to start with.\n",
      "\n",
      "The current model-based algorithm iterates and calculates values such as radius, volume, connection angle of the current structure. \n",
      "\n",
      "My question is, what is the most appropriate ML model for this application? I've looked at GANs but they are imaged based and don't seem to be applicable for extending off of existing data (in this case an initial tree with some branches).\n",
      "I'm currently working on a project in uni where I have to create a model that sorts pictures depending on if they contain sea landscapes or not.\n",
      "\n",
      "A basic intuitive idea was that the features were just the colour histogram of the given picture, but I'm pretty sure it won't be enough.\n",
      "\n",
      "I would like to know how I can count geometrical patterns in a picture, or even if I take some pictures of waves and count on the given picture how many waves.\n",
      "What do you all think some essential ML theory concepts are? Such as bias variance trade off.\n",
      "Hi, object detection noobie here. I want to detect objects using rotated bounding boxes. I labelled them with roLabelImg (rotated boxes version of LabelImg), but I can't feed that data into regular object detection models because there is one more variable (angle) in the annotation file. Is there a way to accomplish this task?\n",
      "Can someone recommend a good paper which I can try to replicate for learning purpose? I am looking for sentiment analysis's use in finance possibly using deep learning models.\n",
      "I have some user-submitted tags that look something like \\[\"Self-hosted\",\"Slaf-hosted\" ... \"self hosted\", \"proprietary\"\\]. I'd like to group them and use an indicator of group membership in a model. Some of the misspellings are kind of gnarly and most of the words are not common. What's a good model for this? Google Universal Sentence Encoder is a couple of years old now and takes a few seconds to return results in REST API mode. Is there something more lightweight or effective that can deal with unseen / misspelled words?\n",
      " \n",
      "\n",
      "**Why is overfitting/ Train on train with Recommenders/MF ok?**\n",
      "\n",
      "In most works with recommender systems (e.g. Matrix factorization models like SVD, ALS, DL + dot product/embedding models etc), the accepted approach seems to be to train a model on the data then get predictions on that same data, for purposes of model evaluation. i.e train a model on the train data, and get predictions from it. This is most common in cases where there's a \"2 step approach\", e.g. a candidate model and a subsequent deeper/slower ranking model (e.g. the Youtube recc model, https://www.tensorflow.org/recommenders/examples/basic\\_ranking etc).\n",
      "\n",
      "There doesn't seem to be much \"support\" as a best practice for doing, say, cross validated predictions on the underlying train data, before training the subsequent model(s) on its output.\n",
      "\n",
      "I understand that MF models \"overfit\"/memorize less than, say, decision trees, but this still seems very odd. You're still training the subsequent model on a different distribution (train on train) than the \"evaluation\" data.\n",
      "\n",
      "Is this just common overfitting, or is there some logic to it?\n",
      "So I have this classification project, you have set of coordinates for each planet relative to earth and based on that we classify, now my question is what would the feature vector look like? since we have 8 planets and each has 3 coordinates, do we concat them in one 1*24 vector or are there any different methods?\n",
      "Hi there,\n",
      "\n",
      "What is going on in the **variables** tab, and why does the tensor data type have so many layers?  (link [here](https://ibb.co/SncSv7k))\n",
      "Hi, is there any research on using images from different datasets for training on a smaller dataset? \n",
      "\n",
      "E.g. I have a class \"dog\" in my custom dataset that only has three images of dogs so I want to augment my training with images from a larger dataset. Is there a way to identify \"good\" images for training?\n",
      "I'm trying to learn English. is there any \"spell checker\" that is able to detect errors like: \"my telephone number is sorry\"?\n",
      "\n",
      "I tried many spell checker but no one is able to dect theese type of errors. Thanks\n",
      "Hi everyone, I am working on a machine learning problem where I have a small dataset of around 1500 examples and working on predicting congestion in IOT networks, is there any optimizer or algorithm or any exisiting dataset which can help me improve accuracy, for now I am achieving 85% and looking to get atleast 90-92%. Any help/leads would be highly helpful.\n",
      "Thanks!\n",
      " Is Nonlinear Gradient Temporal-Difference Learning only for on-policy evaluation? Because there seem no  Importance-weighting terms in the algorithms.\n",
      "Hi, I am an EE student and I am very interested in learning ML; i've been investing time to self-teach myself the concepts and fundamentals of ML. Do you guys have any BEST books recommendation for Maths in Machine Learning?? i'd appreciate it a lot.\n",
      "\n",
      "(note: you can assume I'm a pure beginner in the ML field)\n",
      "Are there any guides that take you step by step into moving objects detection in videos ? Especially I want to know how to convert videos into numbers (I've always worked on well-processed data so I always had matrices of numbers and I just used them) and how to use this data to detect objects... And also if there are any existing libraries that do the job well.\n",
      "\n",
      "Thank you by advance.\n",
      "Anyone knows good transfer learning book with example on how to do my own training, without having to deal with hyper-parameters, custom configuration or confusion matrices?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I have university covid project, in which I need to recognize whether person has medical mask on.\n",
      "\n",
      "The professor wants me to find transfer learning book (not online tutorial, because he feels like book is more credible).\n",
      "\n",
      "But he wants me to do it in brain-dead way: find already trained model (so the model training is fast, and so I don't need to configure) and just feed it my own dataset, using the configuration/schema of the model given in the book.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Anyone has any idea of such a book, which has trained model and only feed it a dataset of people with and without masks (my dataset is labelled)?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Any help is GREATLY appreciated.\n",
      "Is there any advantage in quantizing image inputs? For example, binarized images are sometimes used in mnist training and other character classification problems.\n",
      "\n",
      "Is there any relevant research on any usefulness of using lesser color levels for color images? Like using images with 64 color values as opposed to the typical 256, or something.\n",
      "[deleted]\n",
      "USING APPLE AND NON-APPLE PRODUCTS AT THE SAME TIME\n",
      "\n",
      "I’ve had a MacBook Pro for several years now and it’s time for an upgrade (because the poor thing can barely hold itself together) and I’m considering switching to PC.\n",
      "I have *zero* experience with non-apple products. I just turned 18 and my father is the one who manages the electronics in the house, so he’s always been the one we turn to when purchasing new devices. \n",
      "He’s an Apple guy - so everyone in the house ends up using Apple products.\n",
      "I use an iPhone and recently purchased an iPad \n",
      "I’m concerned about:\n",
      "1. Familiarizing myself with a very different device\n",
      "2. Any \"hardships\" I might have using an iPad and a not-apple computer simultaneously \n",
      "\n",
      "I’m a dummy when it comes to technology pls don’t judge me\n",
      "I'm trying to train a model for a time series of around 2500 said, one data per day and no additional attribute, only the TS. It also has several outliers, but they are not errors in the measurement, they are real.\n",
      "\n",
      "Is it possible to train something good? I keep thinking (and also for the results so far) that the dataset is to small and/or should have some attributes to complement it.\n",
      "Hello all! First post here - very new to machine learning, took a few classes in college so i know the general concepts but am very inexperienced at implementation.\n",
      "\n",
      "I would like to use historical data on higher education institutions with fields such as enrollment, tuition, grants, etc... My goal with this is to see which institutions might have been in distress prior to the impact of covid and was wondering if it would be possible to train an ML model to identify common indicators of distress across a group of schools. How much data would I need (as in historical fiscal years) and what type of model should i look into training? I was thinking maybe an SVM?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Lastly, I feel that using ML might be overkill/ not necessary as I can probably just run normal regressions and correlation analyses, but I would like to attempt it, if anything just for practice.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I appreciate any help, thanks!!\n",
      "\n",
      "Edit: Proficient in Python and R\n",
      "Hello,\n",
      "\n",
      "I'm starting learn ML from scratch. I need a suggestion for my home rig. I'm evaluating a system based on 5900x and a RTX 3080. Is this overkill for a beginner? In the hope I will able to learn enough to start build something useful in the next 12 month, I plan to upgrade my machine later (if it will be possible) by adding one or two more video card, but I'm confused about this point.\n",
      "\n",
      "Does a multi GPU configuration for ML need NVLink (that 3080 miss)? Can I mix different GPU (maybe add a 3080 Ti if and when it will be available)?\n",
      "\n",
      "Thanks\n",
      "Depth as input feature for CNN\n",
      "\n",
      "I  was reading a research paper where  it was mentioned: \"The use of depth  as an input feature for CNNs is  not as well understood as color. How to  take advantage of the rich  information that depth contains remains an  open question.\"\n",
      "\n",
      "Can you all point me to papers which propose CNNs to take advantage of depth as input?\n",
      "I was wondering since we are reaching the point of getting decent images from text generators would it be possible to use it to make better research sistem inside image collection? Like generate an image from the query string and search for similar images inside the database.\n",
      "Would it provide any actual benefits?\n",
      "Does anyone know of a colab for training a BigGAN model using my own labels and dataset?\n",
      "Does anyone have any recommendations for algos to use for signal correlation or matching that aren't DTW?\n",
      "Hey, does anyone have an ETA on Dall-E becoming publicly available?\n",
      "I'm looking for the correct technical term. I'm working on a neuro-evolution simulator, but it is not generation based. There are no distinct generations, the agents reproduce as the simulation runs. I would like to read some scientific papers that use a method like this, but I don't know how to find them. Is there a scientific term for continuous neuro-evolution?\n",
      "I want to learn how to implement a Identity Card verification system. Where can I start and any links to this would be highly appreciated.\n",
      "Hey guys.\n",
      "\n",
      "I was trying to implement Autoencoder architecture for time-series anomaly detection (it actually human anomaly action detection from pose keypoints reconstruction in time). After some initial testing, I have found out that there is almost no difference in results between Autoencoder implemented using LSTM and Fully connected layers... \n",
      "\n",
      "What is the main advantage of LSTM if we have always the same input dimension and sharing of learned representations thought text does not matter?  \n",
      "\n",
      "Also, I have found out that Autoencoder implemented this way is not able to identify some sequences as anomalous, it looks to me almost like it is working in the same way as a series of fully connected layers, ignoring dependencies between features in the same sequence. When I have some sequence, errors from each anomalous point should add up, or it works always purely on basis of compression/reconstruction error?\n",
      "\n",
      "Thank you in advance!\n",
      "Design a Neural Network that solves the problem of facial attribute recognition. The network should receive in the input an image of a face, and should recognise whether the depicted subject wears glasses or not, has long or short hair, smiles or not and should recognise its apparent age. Design the first and the last layers of such a network, detailing your choices. Define the total cost function and give the format of a training example and the corresponding ground truth associated with it. You can treat the recognition of the age either as a regression problem, or as a classification problem – either choice is equally valid.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Can someone help me with this question?\n",
      "&#x200B;\n",
      "\n",
      "Hey all, I'm relatively new to ML and have created a few small projects such as face recognition, object detection, etc on openCV.\n",
      "\n",
      "I wanted to create a model that you can feed images of electric guitars, and have it output the \"Average\" shape of a guitar (with popularity as a weight).\n",
      "\n",
      "Could anyone guide me on the general steps to do this, or similar projects I could look at?\n",
      "Hi, I'm new to this group. I'm trying to extract currency related entities in news headlines. I also want to extend it to a web app to highlight the captured entities. For example the sentence \"Company XYZ gained $100 million in revenue in Q2\". I want to highlight [$100 million] in the headline. Which library can be used to achieve such outcomes? Also note since this is news headlines $ maybe replaced with USD, in that case I would like to highlight [USD 100 million].\n",
      "If someone is wearing a surgical mask, most face recognition software fail to identify the subject's face.\n",
      "\n",
      "Most facial landmark models are trained to identify at least the eyes and the tip of the nose (for example, dlib's 5 point landmark).\n",
      "\n",
      "Is it possible to construct an algorithm/model that is trained to identify a face based on only the eyes?\n",
      "I'm looking to classify (binary label) time series sequence data for about 2000 patients based on 7 different vital signals. What's the best way to store this information as an input vector into a deep learning model?\n",
      "Hey guys.. I'm a grad research student.  I'm working on something novel on anomaly detection for my thesis. The improvement in performance with new method is consistent but very small. Will it still count as some meaningful research? I have to submit a paper based on my work for thesis completion.\n",
      "\n",
      "Any thoughts appreciated..  I'll have to submit my thesis in 2 months.. I can't do much about it either..\n",
      "Hello All:\n",
      "\n",
      "I've been using the super learner model from [https://machinelearningmastery.com/super-learner-ensemble-in-python/](https://machinelearningmastery.com/super-learner-ensemble-in-python/) which runs wonderfully with the randomly generated data set created at the beginning. However, I'm now trying to use it on my own data set and keep getting the following error message: \"ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size **N** and the array at index **X** has size **K**\".  N, X and K represent different values each time the model runs. From what I understand of the code there is an issue within the get\\_out\\_of\\_fold\\_predictions where its attempting to combine the results from all the different models. As each model should be returning a single value for every line item in the test set it feels like it should not be producing arrays of different sizes. The code already includes hstack and vstack. My instinct is that is has something to do with the n\\_splits/random\\_state which is why it works on the rare occasion, that being said I would love any advice on how to correct this error with something more solid than trying every integer in both n\\_splits and random\\_state. Appreciate the help!\n",
      "Hi all,  \n",
      "I'm working with graph neural networks.  \n",
      "I have a large graph. Each node has 4 features \\[A,B,C,D\\]:\n",
      "\n",
      "\\- 2 categorical with high cardinality: 86k (A) and 148k (B) different features\n",
      "\n",
      "\\- 2 integer with ranges: \\[0,4\\] (C) and \\[0,59\\] (D)\n",
      "\n",
      "  \n",
      "After mapping categorical in a set of integers, I encoded a single element with multi-hot encoding in a tensor T where:  \n",
      "length(T) = 86k+148k+5+59\n",
      "\n",
      "and sub tensors:\n",
      "\n",
      "T\\[0:85k\\] sparse that one hot encodes feature A (single 1 in position n for feature n)  \n",
      "T\\[86k:(86k+147k)\\] = T\\[86k:234k\\] one hot encoding feature B  \n",
      "T\\[234k:(234k+5)\\] = T\\[234k:234005\\] one hot encoding feature C  \n",
      "T\\[234005:234064\\] = one hot encoding feature D\n",
      "\n",
      "Does this make any sense? Thanks in advance :D\n",
      "I've  been a professional web developer for a couple of years, but never took  on a project that uses any kind of ML. Now I have a problem that I'd  like to solve with ML, but am having a hard time figuring out what kind  of technology is fit for this purpose.\n",
      "\n",
      "What I want to do is to tag articles by country, based on the headline. A few examples of expected mappings:\n",
      "\n",
      "\"Interview with Martin Jacques on BBC Coverage of China \" => \"China\"  \n",
      "\"The Senate says no to $15\" => \"United States\"  \n",
      "\"Why Memes Will Never Be Monetized \" => \"International\"  \n",
      "\"Along the Thames \" => \"United Kingdom\"  \n",
      "\"Rising Tides: Diving Into Mumbai’s Flooding Challenges \" => \"India\"  \n",
      "\"US Election: What’s at stake for Brazil \" => \"United States, Brazil\"\n",
      "\n",
      "What's a reasonable way to solve this problem? Preferably something I can run myself, without using a paid service\n",
      "If I wanted to use a beta distribution for a VAE on MNIST, which reconstruction loss should I use? (Pytorch)  \n",
      "\n",
      "Since binary cross entropy is meant for bernoulli distributions, but many people seem to use it anyways.\n",
      "Hello all,\n",
      "How to reduce dimension of unseen input features to fit in a model already trained with PCA? E.g Features of single input have dimension of 1×30 and PCA reduces training dataset to x × 22. After the model is trained, how to fit these unseen input features from 30 to 22?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two relativly old papers (by ML timeline standards) by Deepmind :\n",
      "\n",
      " NEVER GIVE UP: LEARNING DIRECTED EXPLORATION STRATEGIES \n",
      "\n",
      "[https://arxiv.org/pdf/2002.06038.pdf](https://arxiv.org/pdf/2002.06038.pdf)\n",
      "\n",
      "Interesting RL paper. The idea is to change the reward value to incite a better exploration. Basicly your agent have two rewards now : the exogene reward (the true reward of the enviromnent) and an intrisic reward (reward that come from the \"novelty\" of the state). It achieves good performance on Atari benchmark. \n",
      "\n",
      " Agent57: Outperforming the Atari Human Benchmark \n",
      "\n",
      "[https://arxiv.org/pdf/2003.13350.pdf](https://arxiv.org/pdf/2003.13350.pdf)\n",
      "\n",
      "From the same authors of the previous paper. It indroduces a lot of improvement of the previous algorithms. The shinning flag is that the algorithm finally achieve to complete all the 57 atari games.\n",
      "\n",
      "There is a lot of comparaison with the Muzero algorithm. I was wondering if you could also apply the \"intrinsec reward\" framework to Muzero too. The aim goal will be to reduce the number of frame to finish the game.\n",
      "This \"random paper prioritization\" scheme has what goal in mind, specifically? This feels like some sort of \"I want to feel ML-savvy and not be left behind\", with no clear application or line of research taken into account.\n",
      "Read older papers (before 2000). You will find many important insights already appeared decades ago. Don't just read papers from certain groups. This leads to groupthink and tends to miss the big picture. Diversify your reading. Read works from even broader fields. Read literature from statistics, robotics, physics, applied math, computer graphics, programming languages, databases, linguistics, HCI.\n",
      "This seems like a fine approach in theory OP, but curating and annotating papers by org and topic is cumbersome.\n",
      "\n",
      "I'm assuming this approach is geared towards keeping up to date with latest research and finding new ideas in different fields. This is what I'd propose as an alternative -\n",
      "\n",
      "1. Pick up a conference/journal for the weekend / week/ any pre-determined period of time \n",
      "\n",
      "2. Look at spotlight papers\n",
      "\n",
      "3. Skim through abstracts for general research ( _improving language models_ is better than _classification of matryoska dolls using x-ray imagery transformed into text_). Try to curate ~10 papers, 7 general, 3 specific.\n",
      "\n",
      "4. Read paper (people already talked about it a lot here, so all I'll say is abstract -> problem statement -> intro -> analysis/results -> approach). \n",
      "\n",
      "5. Read top 1 or 2 references if needed.\n",
      "\n",
      "6. Rinse and repeat till time's up.\n",
      "\n",
      "\n",
      "You're letting reviewers do a majority of the filtering in this case and are still accessing a wide range of information from different fields. As you said, this is something that probably accounts for 10% of your working time, so Id try to keep it as simple as I can with minimal curation effort on my part.\n",
      "I use citations as a proxy for what I'm expected to have read.\n",
      "\n",
      "I try to keep on top of the mega-cited work for general knowledge. For my subfield, I lower the threshold and read everything with a moderate number of citations.\n",
      "\n",
      "Beyond that, I read whatever the heck I feel like to a) have fun and b) broaden my horizons.\n",
      "I have recently started creating [such mind maps](https://imgur.com/a/9fheAiR). I add stuff I find interesting with bit description and links and divide into subcategories. Still new to it but has helped in organising and finding things. For tasks prioritisation and stuff, I have started using Trello.\n",
      "For those whose aim is to follow the very new articles in particular, https://arxivist.com/ might be a thing for you. You log on, upvote some articles you've seen, and the system will give you daily recommendations on what might interest you.\n",
      "You are aware of semantic scholar, right?\n",
      "I read papers as I need them for my research.  If I know nothing about a subject, I look for literature that covers the basics / survey papers. The more I develop my own idea, the more specific papers I will look for and read.\n",
      "\n",
      "  \n",
      "Who just starts reading through 100s/1000s of them at random?\n",
      "> the expected quality of the paper. \"Top\" is for the papers from major institutions (DeepMind, OpenAI, BEAR, etc). \"Others\" is for others. \n",
      "\n",
      "I'm very uncomfortable with this. It may be a heuristic that we unintentionally use, but intentionally using it seems like a recipe to make it even harder for smaller universities to get attention, credit for their ideas, and to compete with FAANG. It's bad enough as it is, let's not exacerbate it.\n",
      "I was playing around with Contrastive Learning for unsupervised image segmentation like a year ago with really good results ( [https://github.com/tpapp157/Contrastive\\_Multiview\\_Coding-Momentum/tree/master/Doom](https://github.com/tpapp157/Contrastive_Multiview_Coding-Momentum/tree/master/Doom) ). Contrastive Learning is just a really powerful technique. It's also a really good secondary objective to add to basically any other architecture to improve training convergence and ensure the model is learning good embeddings (which can then be used for other tasks).\n",
      "Title:TransGAN: Two Transformers Can Make One Strong GAN  \n",
      "\n",
      "Authors:[Yifan Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y), [Shiyu Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+S), [Zhangyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z)  \n",
      "\n",
      "> Abstract: The recent explosive interest on transformers has suggested their potential to become powerful \"universal\" models for computer vision tasks, such as classification, detection, and segmentation. However, how further transformers can go - are they ready to take some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs)? Driven by that curiosity, we conduct the first pilot study in building a GAN \\textbf{completely free of convolutions}, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed \\textbf{TransGAN}, consists of a memory-friendly transformer-based generator that progressively increases feature resolution while decreasing embedding dimension, and a patch-level discriminator that is also transformer-based. We then demonstrate TransGAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy for the generator, and a locally initialized self-attention that emphasizes the neighborhood smoothness of natural images. Equipped with those findings, TransGAN can effectively scale up with bigger models and high-resolution image datasets. Specifically, our best architecture achieves highly competitive performance compared to current state-of-the-art GANs based on convolutional backbones. Specifically, TransGAN sets \\textbf{new state-of-the-art} IS score of 10.10 and FID score of 25.32 on STL-10. It also reaches competitive 8.64 IS score and 11.89 FID score on Cifar-10, and 12.23 FID score on CelebA $64\\times64$, respectively. We also conclude with a discussion of the current limitations and future potential of TransGAN. The code is available at \\url{[this https URL](https://github.com/VITA- Group/TransGAN)}.  \n",
      "\n",
      "[PDF Link](https://arxiv.org/pdf/2102.07074) | [Landing Page](https://arxiv.org/abs/2102.07074) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2102.07074/)\n",
      "Attention has been a part of GANs for a long time, so this is like a logical step.  \n",
      "But I find most methods to encode pixels into transformers to be very ad-hoc and I think it might be improvable in the future.  \n",
      "Especially the limitation of quadratic attention complexity seems to be a huge bottleneck at the moment, which forces patch-wise processing etc.\n",
      "Title:Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm  \n",
      "\n",
      "Authors:[Laria Reynolds](https://arxiv.org/search/cs?searchtype=author&query=Reynolds%2C+L), [Kyle McDonell](https://arxiv.org/search/cs?searchtype=author&query=McDonell%2C+K)  \n",
      "\n",
      "> Abstract: Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. In this work, we discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.  \n",
      "\n",
      "[PDF Link](https://arxiv.org/pdf/2102.07350) | [Landing Page](https://arxiv.org/abs/2102.07350) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2102.07350/)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper explores two ideas. First they use staged training wherein they initially train on short sequences (as in the original bert paper) before then training on full-length 3072-token sequences. They get their best perplexity (a one point improvement) when initially training on sequences of length 64 or 128 for the first 25-100 epochs out of 200 total on Wikitext-103. Looks like it still takes tens of epochs to scale up to longer sequences :/.\n",
      "\n",
      "Second they experiment with encoding position in the key and query vectors only, as in the thumb nail. This part is confusing since they say they reuse the value vector for all three roles. The query and key are *equal* to the value vector in their baseline, and equal to the value vector plus a positional encoding in their new approach. Anyway, if you’re evaluating your language model using a sliding window, you can then get speedups since you don’t need to recompute all the value vectors. The speed gain is only really relevant when generating even more tokens than fit in the context window.\n",
      "\n",
      "I was hoping for a way to get an arbitrarily large context window :/, and for more relevant speed gains. Personally I’m more excited about being able to attend to tens of thousands of tokens than I am about generating tens of thousands of tokens. \n",
      "\n",
      "Oh! Ok I guess the idea is that they’re always training with the sliding window, hence the model learns to hold onto old info and can indirectly attend to it like in an RNN. Hm, I wish they’d discussed this more and used a better benchmark than Wikitext to show it off (most wikitext sequences are pretty short).\n",
      "https://arxiv.org/abs/2012.15832\n",
      "[deleted]\n",
      "🤯 amazing results!\n",
      "This should have been titled\n",
      "\n",
      "> TracIn - Estimating Training Data Influence\n",
      "by Tracing Gradient Descent\n",
      "\n",
      "Them being from Google shouldn't matter and shouldn't be highlighted.\n",
      "The problem you are dealing with is called concept drift (underlying relationships are changing over time), and your second approach is a valid way to deal with it. Is this a deep learning model or some other model trained with a gradient descent based optimizer? That would be ideal. You could fully train on older data, then crawl forward through newer data and continue training until you reach the newest data as your test set. Effectively you are using the model for the older data as transfer learning to initialize the newest model.\n",
      "If you say your old data is significantly different from your current data, what exactly does this mean?\n",
      "- Different set of features and/or outputs?\n",
      "- Different scales of features/outputs\n",
      "- The underlying process (i.e. the function you want to learn) is different?\n",
      "It seems to be a classic lifelong learning problem. The RL community also names it continual learning. There are some methods, among which the elastic weight consolidation may a a promising one.\n",
      "How is this a discussion? It's a link to a video.\n"
     ]
    }
   ],
   "source": [
    "# get 10 hot posts from the MachineLearning subreddit\n",
    "from praw.models import MoreComments\n",
    "hot_posts = reddit.subreddit('MachineLearning').hot(limit=10)\n",
    "for post in hot_posts:\n",
    "    for top_level_comment in post.comments:\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        print(top_level_comment.body)\n",
    "\n",
    "#     print(post.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.11.0-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow_hub) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow_hub) (3.14.0)\n",
      "Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install -q tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /home/jovyan/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /home/jovyan/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
      "label:  0\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:  [b\"Maddy (Debbie Rochon) is a mentally unstable young woman with a troubled past who gets more than she bargained for when she goes to a pool party with a handsome coworker. When her date and his friends jokingly say they belong to a `Murder Club,' Maddy takes it seriously and moves straight up to `Level 3' by bashing in the brains of a woman in a parking garage (for denting her car!). But is Maddy also the one donning a plastic mask and killing off other members of the group or has someone else lost it?<br /><br />The plot of this film (originally titled MAKE 'EM BLEED) is very poorly conceived, full of holes and spirals completely out of control before a ludicrous, out-of-left-field twist ending. Some of the dialogue is downright laughable. I didn't have a problem with Rochon's performance, but the supporting cast was atrocious. However, I managed to sit through this Full Moon release thoroughly entertained. There's plenty of skin and blood and it's the perfect type of flick to sit around with a group of your buddies and pick apart. Horror fans may also enjoy the cameos from Brinke Stevens and Lloyd Kaufman (as Debbie's parents) and Julie Strain (an early victim).<br /><br />Score: 4 out of 10\"\n",
      " b\"CRIME BOSS is directed by Alberto De Martino; an Italian crime drama partially filmed in Hamburg, Germany. An easily forgotten movie. Even in spite of a good car chase sequence, this flick seems to lumber on almost aimlessly. A new Don takes over a powerful Mafia family and finds himself fighting for his own life. Unwritten laws and ethics of the Mafia code make it hard to trust in anyone especially when millions of dollars are at stake. Brutality and violence breed the same in return. This can not be put on a shelf with the real gangster epics. Just the look of the film brings back memories of American drive-in fare. Even the popular American actor Telly Savalas can't boost the calibre of this crime drama. Antonio Sabato also stars with:Paola Tedesco, Guido Lollobrigida, Serio Tramonti and Piero Morgia.\"\n",
      " b'Kubrick proved his brilliantness again, now in a suspense-horror film based on Stephen King\\'s book titled the same way. Jack Torrance is a man in his forties, married, with one child, and with a past of trouble and alcoholism. The Overlook Hotel in Colorado suspends service during the winter because of its extreme weather, and there is a well-paid job for the person who takes care of the facilities during those five months; and Torrance, who was looking to become a writer, found it perfect. But, the manager advised Torrance about the loneliness in this place during the winter, potentially dangerous, and told him that some caretaker in the past went crazy and murdered his family. Even before they got there, his son Danny, who has some sort of imaginary friend who illuminates him the future (shinning), knew the place wasn\\'t good and didn\\'t want to go. Once they installed themselves in the hotel, things started right but within a month, Jack began acting strange, irritated, and depressed. At this point, we know something is going to happen, but don\\'t know when and how. Scary things happen such as the appearance of two twin girls talking to Danny, and someone who attacked him violently. They are not alone in this place. Later on, Jack started to see other people and immediately felt good with them, like if they were his family; among them the famous psychotic caretaker, Delbert Grady. Grady tells Torrance that he must kill his family because they are \"intruders\" in the hotel. Obeying this order, Jack went for the objective and many of the most scary things I\\'ve ever seen happen here. The ending is spectacular and the viewers will stay interested and shocked until the last minute.']\n",
      "\n",
      "labels:  [0 0 1]\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('texts: ', example.numpy()[:3])\n",
    "  print()\n",
    "  print('labels: ', label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
       "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   1,   1, ...,   0,   0,   0],\n",
       "       [861,   1,   7, ...,   0,   0,   0],\n",
       "       [  1,   1,  25, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_example = encoder(example)[:3].numpy()\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00057175]\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text without padding.\n",
    "\n",
    "sample_text = ('The movie was cool. The animation and the graphics '\n",
    "               'were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00057175]\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text with padding\n",
    "\n",
    "padding = \"the \" * 2000\n",
    "predictions = model.predict(np.array([sample_text, padding]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "107/391 [=======>......................] - ETA: 3:40 - loss: 0.6928 - accuracy: 0.4933"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7944b517869f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(train_dataset, epochs=10,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     validation_steps=30)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset, \n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
